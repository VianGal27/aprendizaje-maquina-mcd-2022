[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aprendizaje Máquina",
    "section": "",
    "text": "Todas las notas y material del curso estarán en este repositorio.\n\nIntroducción al aprendizaje máquina\nMétodos locales y regresión lineal\nIngeniería de variables de entrada\nRegularización\nProblemas de clasificación y regresión logística\nMétodos de remuestreo y validación cruzada\nRedes neuronales\nÁrboles, bosques aleatorios y boosting\nDiagnóstico y mejora en problemas de aprendizaje supervisado\nComponentes principales y análisis de conglomerados\n\n\n\n\nTareas semanales (20%)\nExamen parcial (30% práctico, 20% teórico)\nUn examen final (30% práctico)\n\n\n\n\nCada semestre las notas cambian, en algunas partes considerablemente. Las de este semestre están en este repositorio, incluyendo ejemplos, ejercicios y tareas.\n\n\n\n\nAn Introduction to Statistical Learning, James et al. (2014)\nDeep Learning, Goodfellow, Bengio, y Courville (2016)\nTidy Modeling with R, Kuhn y Silge (2022)\n\n\n\n\n\nPattern Recognition and Machine Learning, Bishop (2006)\nThe Elements of Statistical Learning, Hastie, Tibshirani, y Friedman (2017)\nPredicción conformehttps://people.eecs.berkeley.edu/~angelopoulos/blog/posts/gentle-intro/\n\n\n\n\nPara hacer las tareas y exámenes pueden usar cualquier lenguaje o flujo de trabajo que les convenga (R o Python, por ejemplo) - el único requisito esté basado en código y no point-and-click. En lo posible utilizamos librerías especializadas que se pueden utilizar desde varias plataformas (keras, por ejemplo).\n\nR Sitio de R (CRAN)\nRstudio Interfaz gráfica para trabajar en R.\nRecursos para aprender R\n\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Secaucus, NJ, USA: Springer-Verlag New York, Inc.\n\n\nGoodfellow, Ian, Yoshua Bengio, y Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nHastie, Trevor, Robert Tibshirani, y Jerome Friedman. 2017. The Elements of Statistical Learning. Springer Series en Statistics. Springer New York Inc. http://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, y Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Incorporated. http://www-bcf.usc.edu/~gareth/ISL/.\n\n\nKuhn, M., y J. Silge. 2022. Tidy Modeling with R. O’Reilly Media. https://books.google.com.mx/books?id=9cJ6EAAAQBAJ."
  },
  {
    "objectID": "01-introduccion.html",
    "href": "01-introduccion.html",
    "title": "1  Introducción",
    "section": "",
    "text": "Métodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión.\nEn este curso nos enfocamos en las tareas de aprendizaje supervisado (predecir o estimar una variable respuesta a partir de datos de entrada) y aprendizaje no supervisado (describir estructuras interesantes en datos, donde no necesariamente hay una respuesta que predecir). Existe también aprendizaje por refuerzo, en donde buscamos aprender a tomar decisiones en un entorno en donde la decisión afecta directa e inmediatamente al entorno.\n\n\n\nPredecir si un cliente de tarjeta de crédito va a caer en impago en los próximos tres meses.\nReconocer palabras escritas a mano (OCR).\nDetectar llamados de ballenas en grabaciones de boyas.\nEstimar el ingreso mensual de un hogar a partir de las características de la vivienda, posesiones y equipamiento y localización geográfica.\nDividir a los clientes de Netflix según sus gustos.\nRecomendar artículos a clientes de un programa de lealtad o servicio online.\n\nLas razones usuales para intentar resolver estos problemas computacionalmente son diversas:\n\nQuisiéramos obtener una respuesta barata, rápida, automatizada, y con suficiente precisión. Por ejemplo, reconocer caracteres en una placa de coche de una fotografía se puede hacer por personas, pero eso es lento y costoso. Igual oír cada segundo de grabación de las boyas para saber si hay ballenas o no. Hacer mediciones directas del ingreso de un hogar requiere mucho tiempo y esfuerzo.\nQuisiéramos superar el desempeño actual de los expertos o de reglas simples utilizando datos: por ejemplo, en la decisión de dar o no un préstamo a un solicitante, puede ser posible tomar mejores decisiones con algoritmos que con evaluaciones personales o con reglas simples que toman en cuenta el ingreso mensual, por ejemplo.\nAl resolver estos problemas computacionalmente tenemos oportunidad de aprender más del problema que nos interesa: estas soluciones forman parte de un ciclo de análisis de datos donde podemos aprender de una forma más concentrada cuáles son características y patrones importantes de nuestros datos.\n\nEs posible aproximarse a todos estos problemas usando reglas (por ejemplo, si los pixeles del centro de la imagen están vacíos, entonces es un cero, si el crédito total es mayor al 50% del ingreso anual, declinar el préstamo, etc). Las razones para no tomar un enfoque de reglas construidas “a mano”:\n\nCuando conjuntos de reglas creadas a mano se desempeñan mal (por ejemplo, para otorgar créditos, reconocer caracteres, etc.)\nReglas creadas a mano pueden ser difíciles de mantener (por ejemplo, un corrector ortográfico), pues para problemas interesantes muchas veces se requieren grandes cantidades de reglas. Por ejemplo: ¿qué búsquedas www se enfocan en dar direcciones como resultados? ¿cómo filtrar comentarios no aceptables en foros?"
  },
  {
    "objectID": "01-introduccion.html#ejemplo-reglas-y-aprendizaje",
    "href": "01-introduccion.html#ejemplo-reglas-y-aprendizaje",
    "title": "1  Introducción",
    "section": "1.2 Ejemplo: reglas y aprendizaje",
    "text": "1.2 Ejemplo: reglas y aprendizaje\nLectura de un medidor mediante imágenes. Supongamos que en una infraestructura donde hay medidores análogos (de agua, electricidad, gas, etc.) que no se comunican. ¿Podríamos pensar en utilizar fotos tomadas automáticamente para medir el consumo?\nPor ejemplo, consideramos el siguiente problema (tomado de aquí, ver código y datos):\n\nlibrary(imager)\nlibrary(tidyverse)\nlibrary(gt)\nset.seed(43)\npath_img <- \"../datos/medidor/\"\npath_full_imgs <- list.files(path = path_img, full.names = TRUE)\nmedidor <- load.image(sample(path_full_imgs, 1))\npar(mar = c(1, 1, 1, 1))\nplot(medidor, axes = FALSE)\n\n\n\n\nNótese que las imágenes y videos son matrices o arreglos de valores de pixeles, por ejemplo estas son las dimensiones para una imagen:\n\ndim(medidor)\n\n[1] 193 193   1   3\n\n\nEn este caso, la imagen es de 193 x 193 pixeles y tiene tres canales, o tres matrices de 193 x 193 donde la entrada de cada matriz es la intensidad del canal correspondiente. Buscámos hacer cálculos con estas matrices para extraer la información que queremos. En este caso, construiremos estos cálculos a mano.\nPrimero filtramos (extraemos canal rojo y azul, restamos, difuminamos y aplicamos un umbral):\n\nmedidor_rojo <- medidor |>  R() \nmedidor_azul <- medidor |> B()\nmedidor_1 <- (medidor_rojo - medidor_azul) |> isoblur(5)\naguja <-  medidor_1 |>  imager::threshold(\"90%\", approx = FALSE)\n\n\n\n\n\n\nLogramos extraer la aguja, aunque hay algo de ruido adicional. Una estrategia es extraer la componente conexa más grande (que debería corresponder a la aguja), y luego calcular su orientación. Una manera fácil es encontrar una recta que vaya del centro de la imagen hasta el punto más alejado del centro (aunque quizá puedes pensar maneras más robustas de hacer esto):\n\ncalcular_punta <- function(pixset){\n  centro <- floor(dim(pixset)[1:2] / 2)\n  # segmentar en componentes conexas\n  componentes <- split_connected(pixset)\n  # calcular la más grande\n  num_pixeles <- map_dbl(componentes, sum)\n  ind_maxima <- which.max(num_pixeles)\n  pixset_tbl <- as_tibble(componentes[[ind_maxima]]) |> \n    mutate(dist = (x - centro[1])^2 + (y - centro[2])^2) |> \n    top_n(1, dist)  |> \n    mutate(x_1 = x - centro[1], y_1 = y - centro[2])\n  pixset_tbl[1, ] \n}\n\n\n\n\n\n\nY ahora podemos aplicar el proceso de arriba a todas la imágenes:\n\npath_imgs <- list.files(path = path_img)\n\npath_full_imgs <- list.files(path = path_img, full.names = TRUE)\n# en este caso los datos están etiquetados\ny_imagenes <- path_imgs |> str_sub(1, 3) |> as.numeric()\n# procesar algunas imagenes\nset.seed(82)\nindice_imgs <- sample(1:length(path_full_imgs), 500)\nangulos <- path_full_imgs[indice_imgs] |> \n    map( ~ load.image(.x)) |>  \n    map(~ R(.x) - B(.x)) |> \n    map( ~ isoblur(.x, 5)) |> \n    map( ~ imager::threshold(.x, \"90%\")) |> \n    map( ~ calcular_punta(.x)) |> \n  bind_rows()\n\n\nangulos_tbl <- angulos |> mutate(y_medidor = y_imagenes[indice_imgs])\nggplot(angulos_tbl, aes(x = 180 * atan2(y_1, x_1) / pi + 90, y = y_medidor)) +\n  geom_point() + xlab(\"Ángulo\")\n\n\n\n\nEl desempeño no es muy malo pero tiene algunas fallas grandes. Quizá refinando nuestro pipeline de procesamiento podemos mejorarlo.\n\nPor el contrario, en el enfoque de aprendizaje, comenzamos con un conjunto de datos etiquetado (por una persona, por un método costoso, etc.), y utilizamos alguna estructura general para aprender a producir la respuesta a partir de las imágenes. Por ejemplo, en este caso podríamos una red convolucional sobre los valores de los pixeles de la imagen:\n\nreticulate::use_virtualenv(\"/opt/virtualenvs/mi_env\")\nlibrary(keras)\n# usamos los tres canales de la imagen\nimagenes <- map(path_full_imgs, ~ image_load(.x, target_size = c(64, 64)))\n\nLoaded Tensorflow version 2.6.0\n\nimgs_array <-  imagenes |>  map(~ image_to_array(.x)) \nimgs_array <- map(imgs_array, ~ array_reshape(.x, c(1, 64, 64, 3)))\nx <- abind::abind(imgs_array, along = 1)\nset.seed(2311)\nindices_entrena <- sample(1:dim(x)[1], size = 4200)\n# generar lotes de datos de las imágenes originales\ngenerador_1 <- image_data_generator(\n  rescale = 1/255,\n  rotation_range = 5,\n  zoom_range = 0.05,\n  horizontal_flip = FALSE,\n  vertical_flip = FALSE,\n  fill_mode = \"nearest\"\n)\ngenerador_entrena <- flow_images_from_data(\n  x = x[indices_entrena,,,],\n  y = y_imagenes[indices_entrena] / 10,\n  generator = generador_1,\n  shuffle = TRUE,\n  batch_size = 32\n)\n\n\nmodelo_aguja <- keras_model_sequential() |>\n  layer_conv_2d(input_shape = c(64, 64, 3), filters = 32, kernel_size = c(5, 5)) |> \n  layer_max_pooling_2d(pool_size = c(2, 2)) |>\n  layer_conv_2d(filters = 32, kernel_size = c(5, 5)) |> \n  layer_max_pooling_2d(pool_size = c(2, 2)) |> \n  layer_conv_2d(filters = 16, kernel_size = c(3, 3)) |> \n  layer_max_pooling_2d(pool_size = c(2, 2)) |> \n  layer_flatten() |> \n  layer_dropout(0.2) |> \n  layer_dense(units = 100, activation = \"sigmoid\") |>\n  layer_dropout(0.2) |> \n  layer_dense(units = 1, activation = 'linear')\n\nAjustamos el modelo:\n\nmodelo_aguja |> compile(\n  loss = \"mse\",\n  optimizer = optimizer_adam(lr = 0.001),\n  metrics = c('mae')\n)                                                                                                        \n# Entrenar\nmodelo_aguja |> fit(\n  generador_entrena,\n  epochs = 80,\n  verbose = TRUE, \n  validation_data = list(x = x[-indices_entrena,,,], \n                         y = y_imagenes[-c(indices_entrena)] / 10)\n)\nsave_model_hdf5(modelo_aguja, \"cache/modelo-aguja.h5\")\n\n\nmodelo <- load_model_hdf5(\"cache/modelo-aguja.h5\")\nmodelo\n\nModel: \"sequential\"\n________________________________________________________________________________\nLayer (type)                        Output Shape                    Param #     \n================================================================================\nconv2d_2 (Conv2D)                   (None, 60, 60, 32)              2432        \n________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)      (None, 30, 30, 32)              0           \n________________________________________________________________________________\nconv2d_1 (Conv2D)                   (None, 26, 26, 32)              25632       \n________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)      (None, 13, 13, 32)              0           \n________________________________________________________________________________\nconv2d (Conv2D)                     (None, 11, 11, 16)              4624        \n________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)        (None, 5, 5, 16)                0           \n________________________________________________________________________________\nflatten (Flatten)                   (None, 400)                     0           \n________________________________________________________________________________\ndropout_1 (Dropout)                 (None, 400)                     0           \n________________________________________________________________________________\ndense_1 (Dense)                     (None, 100)                     40100       \n________________________________________________________________________________\ndropout (Dropout)                   (None, 100)                     0           \n________________________________________________________________________________\ndense (Dense)                       (None, 1)                       101         \n================================================================================\nTotal params: 72,889\nTrainable params: 72,889\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nY observamos que obtenemos predicciones prometedoras:\n\npreds <- predict(modelo, x[-indices_entrena,,,])\npreds_tbl <- tibble(y = y_imagenes[-c(indices_entrena)] / 10, preds = preds)\nggplot(preds_tbl, aes(x = preds, y = y)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(colour = 'red')\n\n\n\n\nDe forma que podemos resolver este problema con algoritmos generales, sin tener que aplicar métodos sofisticados de procesamiento de imágenes. El enfoque de aprendizaje es particularmente efectivo cuando hay cantidades grandes de datos poco ruidosos, y aunque en este ejemplo los dos enfoques dan resultados razonables, en procesamiento de imágenes es cada vez más común usar redes neuronales grandes para resolver este tipo de problemas."
  },
  {
    "objectID": "01-introduccion.html#medicioncostosa",
    "href": "01-introduccion.html#medicioncostosa",
    "title": "1  Introducción",
    "section": "1.3 Ejemplo: mediciones costosas",
    "text": "1.3 Ejemplo: mediciones costosas\nEn algunos casos, el estándar de la medición que nos interesa es uno que es costoso de cumplir: a veces se dice que etiquetar los datos es costoso. Un ejemplo es producir las estimaciones de ingreso trimestral de un hogar que se recolecta en la ENIGH (ver aquí). En este caso particular, se utiliza esta encuesta como datos etiquetados para poder estimar el ingreso de otros hogares que no están en la muestra del ENIGH, pero para los que se conocen características de las vivienda, características de los integrantes, y otras medidas que son más fácilmente recolectadas en encuestas de opinión.\nVeremos otro ejemplo: estimar el valor de mercado de las casas en venta de una región. Es posible que tengamos un inventario de casas con varias de sus características registradas, pero producir estimaciones correctas de su valor de mercado puede requerir de inspecciones costosas de expertos, o tomar aproximaciones imprecisas de esta cantidad (por ejemplo, cuál es el precio ofertado).\nUtilizaremos datos de casas que se vendieron en Ames, Iowa en cierto periodo. En este caso, conocemos el valor a la que se vendió una casa. Buscamos producir una estimación para otras casas para las cuales conocemos características como su localización, superficie en metros cuadrados, año de construcción, espacio de estacionamiento, y así sucesivamente. Estas medidas son más fáciles de recolectar, y quisiéramos producir una estimación de su precio de venta en términos de estas medidas.\nEn este ejemplo intentaremos una forma simple de predecir.\n\nlibrary(tidymodels)\nlibrary(patchwork)\nsource(\"../R/casas_traducir_geo.R\")\nset.seed(68821)\n# dividir muestra\ncasas_split <- initial_split(casas, prop = 0.75)\n# obtener muestra de entrenamiento\ncasas_entrena <- training(casas_split)\n# graficar\ng_1 <- ggplot(casas_entrena, aes(x = precio_miles)) +\n  geom_histogram()\ng_2 <- ggplot(casas_entrena, aes(x = area_hab_m2, \n                          y = precio_miles, \n                          colour = condicion_venta)) +\n  geom_point() \ng_1 + g_2\n\n\n\n\nLa variable de condición de venta no podemos utilizarla para predecir, pues sólo la conocemos una vez que la venta se hace. Podemos ver en lugar de eso solamente las de condición normal. Consideramos además del área habitable, por ejemplo, la calidad general de terminados:\n\ng_1 <- ggplot(casas_entrena |>  \n         filter(condicion_venta == \"Normal\") |>  \n         mutate(calidad_grupo = cut(calidad_gral, breaks = c(0, 5, 7, 8, 10))), \n       aes(x = area_hab_m2, \n           y = precio_miles,\n           colour = calidad_grupo)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, formula = \"y ~ x\")\ng_1\n\n\n\n\nPrecio vs área y calidad\n\n\n\n\nVemos que estas dos variables que hemos usado explican buena parte de la variación de los precios de las casas. Podemos examinar otras variables como la existencia y tamaño del garage:\n\nggplot(casas_entrena |>  filter(condicion_venta == \"Normal\"),\n       aes(x = area_hab_m2, y = precio_miles, colour = area_garage_m2)) +\n  geom_point(alpha = 0.5) + facet_wrap(~ (area_garage_m2 == 0))\n\n\n\n\nY quizá podríamos proponer una fórmula simple de la forma:\n\\[Precio = a_{calidad} + b_{calidad}\\textrm{Area} + c \\textrm{AreaGarage} + d\\textrm{TieneGarage}\\]\ndonde los valores de \\(a_{calidad}, b_{calidad}, c, d\\) podríamos estimarlos de los datos. La pendiente de Area dependende de la calificación de la calidad de los terminados.\nNuestro proceso comenzaría entonces construir los datos para usar en el modelo:\n\nreceta_casas <- \n  recipe(precio_miles ~ area_hab_m2 + calidad_gral + \n           area_garage_m2, \n         data = casas_entrena) |>  \n  step_cut(calidad_gral, breaks = c(3, 5, 6, 7, 8)) |>  \n  step_normalize(starts_with(\"area\")) |> \n  step_mutate(tiene_garage = ifelse(area_garage_m2 > 0, 1, 0)) |> \n  step_dummy(calidad_gral) |> \n  step_interact(terms = ~ area_hab_m2:starts_with(\"calidad_gral\")) \n\nDefinimos el tipo de modelo que queremos ajustar, creamos un flujo y ajustamos\n\n# modelo\ncasas_modelo <- linear_reg() |> \n  set_engine(\"lm\")\n# flujo\nworkflow_casas <- workflow() |> \n  add_recipe(receta_casas) |> \n  add_model(casas_modelo)\n# ajustar flujo\najuste <- fit(workflow_casas, casas_entrena)\najuste\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_cut()\n• step_normalize()\n• step_mutate()\n• step_dummy()\n• step_interact()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n                       (Intercept)                         area_hab_m2  \n                           111.124                              22.505  \n                    area_garage_m2                        tiene_garage  \n                            12.014                               3.230  \n               calidad_gral_X.3.5.                 calidad_gral_X.5.6.  \n                            30.104                              54.623  \n               calidad_gral_X.6.7.                 calidad_gral_X.7.8.  \n                            79.565                             119.639  \n              calidad_gral_X.8.10.   area_hab_m2_x_calidad_gral_X.3.5.  \n                           217.099                              -7.942  \n area_hab_m2_x_calidad_gral_X.5.6.   area_hab_m2_x_calidad_gral_X.6.7.  \n                             2.839                              14.141  \n area_hab_m2_x_calidad_gral_X.7.8.  area_hab_m2_x_calidad_gral_X.8.10.  \n                            14.221                              -1.421  \n\n\nY ahora podemos hacer predicciones:\n\nset.seed(8)\ncasas_prueba <- testing(casas_split) \nejemplos <- casas_prueba|> sample_n(5)\npredict(ajuste, ejemplos) |> \n  bind_cols(ejemplos |> select(precio_miles, area_hab_m2)) |> \n  arrange(desc(precio_miles)) |> gt() |> \n  fmt_number(columns = everything(), decimals = 1)\n\n\n\n\n\n  \n  \n    \n      .pred\n      precio_miles\n      area_hab_m2\n    \n  \n  \n    242.3\n275.0\n152.9\n    177.3\n181.0\n155.6\n    169.3\n175.5\n132.1\n    123.1\n133.0\n117.8\n    115.6\n128.5\n90.2\n  \n  \n  \n\n\n\n\nY finalmente podemos evaluar nuestro modelo. En este casos mostramos diversas métricas como ejemplo:\n\nmetricas <- metric_set(mape, mae, rmse)\nmetricas(casas_prueba |> bind_cols(predict(ajuste, casas_prueba)), \n     truth = precio_miles, estimate = .pred) |> gt() |> \n  fmt_number(columns = where(is_double), decimals = 1)\n\n\n\n\n\n  \n  \n    \n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    mape\nstandard\n14.1\n    mae\nstandard\n23.4\n    rmse\nstandard\n33.3\n  \n  \n  \n\n\n\n\n\ncasas_prueba_f <- filter(casas_prueba,\n  condicion_venta %in% c(\"Normal\", \"Partial\", \"Abnorml\"))\nggplot(casas_prueba_f |>\n       bind_cols(predict(ajuste, casas_prueba_f)),\n       aes(x = .pred, y = precio_miles)) +\n  geom_point() +\n  geom_abline(colour = \"red\") + facet_wrap(~ condicion_venta)\n\n\n\n\nEste modelo tiene algunos defectos y todavía tiene error considerablemente grande. La mejora sin embargo podemos cuantificarla con un modelo base o benchmark. En este caso utilizamos el siguiente modelo simple, cuya predicción es el promedio de entrenamiento:\n\n# nearest neighbors es grande, así que la predicción\n# es el promedio de precio en entrenamiento\ncasas_promedio <- nearest_neighbor(\n    neighbors = 1000, weight_func = \"rectangular\") |>\n  set_mode(\"regression\") |> \n  set_engine(\"kknn\")\nworkflow_base <- workflow() |> \n  add_recipe(receta_casas) |> \n  add_model(casas_promedio)\najuste_base <- fit(workflow_base, casas_entrena)\nmetricas(casas_prueba |> bind_cols(predict(ajuste_base, casas_prueba)), \n     truth = precio_miles, estimate = .pred)|> gt() |> \n  fmt_number(columns = where(is_double), decimals = 1)\n\n\n\n\n\n  \n  \n    \n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    mape\nstandard\n33.4\n    mae\nstandard\n54.8\n    rmse\nstandard\n77.2"
  },
  {
    "objectID": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "href": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "title": "1  Introducción",
    "section": "1.4 Aprendizaje supervisado y no supervisado",
    "text": "1.4 Aprendizaje supervisado y no supervisado\nLas tareas de aprendizaje se dividen en dos grandes partes: aprendizaje supervisado y aprendizaje no supervisado.\nEn Aprendizaje supervisado buscamos construir un modelo o algoritmo para predecir o estimar un target o una respuesta a partir de ciertas variables de entrada.\nPredecir y estimar, en este contexto, se refieren a cosas similares. Generalmente se usa predecir cuando se trata de variables que no son observables ahora, sino en el futuro, y estimar cuando nos interesan variables actuales que no podemos observar ahora por costos o por la naturaleza del fenómeno.\nPor ejemplo, para identificar a los clientes con alto riesgo de impago de tarjeta de crédito, utilizamos datos históricos de clientes que han pagado y no han pagado. Con estos datos entrenamos un algoritmo para detectar anticipadamente los clientes con alto riesgo de impago.\nUsualmente dividimos los problemas de aprendizaje supervisado en dos tipos, dependiendo de la variables salida:\n\nProblemas de regresión: cuando la salida es una variable numérica. El ejemplo de estimación de ingreso es un problema de regresión\nProblemas de clasificación: cuando la salida es una variable categórica. El ejemplo de detección de dígitos escritos a manos es un problema de clasificación.\n\nEn contraste, en Aprendizaje no supervisado no hay target o variable respuesta. Buscamos modelar y entender las relaciones entre variables y entre observaciones, o patrones importantes o interesantes en los datos.\nLos problemas supervisados tienen un objetivo claro: hacer las mejores predicciones posibles bajo ciertas restricciones. Los problemas no supervisados tienden a tener objetivos más vagos, y por lo mismo pueden ser más difíciles."
  },
  {
    "objectID": "02-principios-supervisado.html",
    "href": "02-principios-supervisado.html",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "",
    "text": "En esta sección examinaremos algunos principios teóricos y de metodología para el aprendizaje supervisado."
  },
  {
    "objectID": "02-principios-supervisado.html#población-y-pérdida",
    "href": "02-principios-supervisado.html#población-y-pérdida",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.1 Población y pérdida",
    "text": "2.1 Población y pérdida\nSupongamos que tenemos una población grande de observaciones potenciales de la forma\n\\[(x_1, x_2, \\ldots, x_p, y) \\]\nY para esa población nos interesa predecir una variable respuesta \\(y\\) numérica en términos de variables de entrada disponibles \\(x = (x_1,x_2,\\ldots, x_p)\\):\n\\[(x_1, x_2, \\ldots, x_p) \\to y\\]\nEl proceso que produce la salida \\(y\\) a partir de las entradas es típicamente muy complejo y dificíl de describir de forma mecanística (por ejemplo, el ingreso dadas características de los hogares).\n\nEjemplo\nPara ilustrar esta discusión teórica, consideraremos datos simulados. La población está dada por el siguiente proceso generador de datos:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(gt)\ngenera_datos <- function(n = 500, tipo = NULL){\n  dat_tbl <- tibble(nse = runif(n, 0, 100)) |>\n    mutate(estudio_años = floor(rnorm(n, 1.5 * sqrt(nse), 1))) |>\n    mutate(estudio_años = pmax(0, pmin(17, estudio_años))) |> \n    mutate(habilidad = rnorm(n, 100 + 0.1 * nse, 1)) |> \n    mutate(z = 100 + (habilidad/100) * ( 20 * nse + 5 * (estudio_años))) |> \n    mutate(ingreso = pmax(0, 0.2*(z + rnorm(n, 0, 150))))\n  obs_tbl <- dat_tbl |> \n    mutate(tipo = tipo, id = 1:n)\n  obs_tbl |> select(id, tipo, x = estudio_años, y = ingreso)\n}\n\nTenemos una sola entrada y una respuesta numérica, y una muestra se ve como sigue:\n\nset.seed(1234)\ndatos_tbl <- genera_datos(n = 500, tipo = \"entrena\")\nggplot(datos_tbl, aes(x = x, y = y)) + geom_jitter(width = 0.3) +\n  xlab(\"Años de estudio\") + ylab(\"Ingreso anual (miles)\")\n\n\n\n\n\nBuscamos construir una función \\(f\\) tal que si observamos cualquier \\(x = (x_1, x_2, \\ldots, x_p) \\to y\\), entonces nuestra predicción es\n\\[\\hat{y} = f(x_1, x_2, \\ldots, x_p) = f(x).\\]\nCon esta regla o algoritmo \\(f\\) queremos predecir con buena precisión el valor de \\(y\\). Esta \\(f\\), como explicamos antes, puede ser producida de muy distintas maneras (experiencia, reglas a mano, datos, etc.)\nNuestra primera tarea es definir qué quiere decir predecir con buena precisión.\nPara hacer esto tenemos que introducir una medida del error, que llamamos en general función de pérdida.\n\n\n\n\n\n\nFunción de pérdida y error de predicción\n\n\n\nSi el verdadero valor observado es \\(y\\) y nuestra predicción es \\(f(x)\\), denotamos la pérdida asociada a esta observación como\n\\[L(y, f(x))\\]\nPara medir el desempeño general de la regla \\(f\\), consideramos su valor esperado, el error de predicción, que es el promedio sobre toda la población:\n\\[Err(f) = E[L(y, f(x))]\\]\nEste es el error que obtendríamos promediando las pérdidas sobre toda la población de interés.\n\n\nObservación: Para fijar ideas, podríamos usar por ejemplo la pérdida absoluta \\(L(y, f(x)) = |y - f(x)|\\) o la pérdida cuadrática \\(L(y, f(x)) = (y - f(x))^2\\).\nAl menos en teoría, podemos encontrar una \\(f\\) que minimiza esta pérdida:\n\n\n\n\n\n\nPredictor óptimo\n\n\n\nPara una población dada, el predictor óptimo (teórico) es\n\\[f^* = \\underset{f}{\\mathrm{argmin}} E[L(y, f(x))].\\]\nEs decir: el mínimo error posible que podemos obtener es \\(Err(f^*)\\). Para cualquier otro predictor \\(f\\) tenemos que \\(Err(f) \\geq Err(f^*).\\)\n\n\nPor ejemplo si usamos la pérdida cuadrática \\(L(y, f(x)) = (y - f(x))^2\\), entonces puede mostrarse que\n\\[f^*(x) = E(y | x)\\]\nde forma que \\(f^*\\) es la media condicional de la \\(y\\) dado que sabemos que las entradas son \\(x\\). Si usáramos la pérdida absoluta \\(L(y, f(x)) = |y - f(x)|\\) entonces\n\\[f^*(x) = \\textrm{mediana}(y|x).\\]\nDistintas funciones de pérdida dan distintas soluciones teóricas. Por ejemplo, si existen valores atípicos en \\(y\\) producidos por errores de registro o medición, usar la pérdida absoluta puede dar mejores resultados que la cuadrática, que tiende a dar mayor peso a errores grandes.\nObservaciones:\n\nPodemos ver nuestra tarea entonces como una de ajuste de curvas: queremos aproximar tan bien como sea posible la función \\(f^*(x)\\).\nNo es simple decidir qué función de pérdida debería utilizarse para un problema dado de predicción.\nGeneralmente es una combinación de costos/beneficios del problema que tratamos, conveniencia computacional, y cómo se comportan los errores de nuestros predictores bajo distintas pérdidas.\nMuchas veces es mejor considerar el problema de selección de la pérdida desde dos ángulos: el primero es computacional y de propiedades de la predicción, y el segundo tiene que ver con costos y beneficios asociados al problema que queremos resolver. Para el primero, alguna de las pérdidas estándar (como las que vimos arriba, cuadrática y absoluta, ologarítmica) son usualmente suficiente. En el segundo enfoque, el análisis es generalmente involucra más aspectos particulares del problema y generalmente tiene que hacerse de manera ad-hoc.\n\n\n\nEjemplo\nSupongamos que nos interesa minimizar la pérdida cuadrática. Si tomamos una muestra muy grande (para este problema), podemos aproximar la predicción óptima directamente. Abajo graficamos nuestra muestra chica de datos junto con una buna aproximación del predictor óptimo:\n\npoblacion_tbl <- genera_datos(n = 50000, tipo = \"poblacion\")\n# calcular óptimo\npreds_graf_tbl <- poblacion_tbl |> \n  group_by(x) |> # condicionar a x\n  summarise(.pred = mean(y)) |> # mediana, pues usamos pérdida absoluta\n  mutate(predictor = \"_óptimo\")\n# graficar con una muestra grande\nggplot(datos_tbl, aes(x = x)) +\n  geom_jitter(aes(y = y), colour = \"red\") + \n  geom_line(data = preds_graf_tbl, aes(y = .pred, colour = predictor), size = 1.1) +\n  xlab(\"Años de estudio\") + ylab(\"Ingreso anual (miles)\")"
  },
  {
    "objectID": "02-principios-supervisado.html#estimando-el-desempeño-y-datos-de-prueba",
    "href": "02-principios-supervisado.html#estimando-el-desempeño-y-datos-de-prueba",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.2 Estimando el desempeño y datos de prueba",
    "text": "2.2 Estimando el desempeño y datos de prueba\nPara obtener una estimación de la pérdida para una función \\(f\\) que usamos para hacer predicciones, podemos tomar una muestra de datos del proceso generador:\n\\[{\\mathcal T} = \\{(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}), (\\mathbf{x}^{(2)}, \\mathbf{y}^{(2)}), \\ldots, (\\mathbf{x}^{(m)}, \\mathbf{y}^{(m)})\\},\\]\nCompararíamos entonces las respuestas observadas \\(\\mathbf{y^{(i)}}\\) con las predicciones \\(f(\\mathbf{x^{(i)}})\\). Ahora resumimos evaluando el error promedio sobre los datos de prueba. El error de prueba de \\(f\\) es\n\\[ \\widehat{Err}(f) = \\frac{1}{m} \\sum_{i=1}^m L(\\mathbf{y}^{(i)} , f(\\mathbf{x}^{(i)}))\\]\nPor ejemplo, si usamos la pérdida cuadrática,\n\\[ \\widehat{Err}(f) = \\frac{1}{m} \\sum_{i=1}^m (\\mathbf{y}^{(i)} - f(\\mathbf{x}^{(i)}))^2\\]\nSi \\(m\\) es grande, entonces tenemos por la ley de los grandes números que\n\\[Err(f) \\approx \\widehat{Err} (f)\\]\nPodemos también estimar el error de estimación de \\(\\widehat{Err}(f)\\) con técnicas estándar, por ejemplo bootstrap o aproximación normal.\nObervación: nótese que en estos cálculos no es necesario hacer ningún supuesto acerca de \\(f\\), que en este argumento está fija y no utiliza la muestra de prueba.\n\nEjemplo: óptimo\nSupongamos que \\(f\\) es el predictor óptimo que obtuvimos arriba (pero esto aplica para cualquier otra función \\(f\\) que usemos para hacer predicciones). Tomamos una muestra de prueba, y evaluamos usando la raíz de la pérdida cuadrática media:\n\nprueba_tbl <- genera_datos(n = 2000, tipo = \"prueba\")\neval_tbl <- prueba_tbl |>  \n  left_join(preds_graf_tbl, by = \"x\") \nresumen_tbl <- eval_tbl |>  \n  group_by(predictor, tipo) |> \n  rmse(truth = y, estimate = .pred) \nfmt_resumen <- function(resumen_tbl){\n  resumen_tbl |> \n    select(-.estimator) |> \n    pivot_wider(names_from = tipo, values_from = .estimate) |> \n    gt() |> \n    fmt_number(where(is_double), decimals = 0)\n}\nfmt_resumen(resumen_tbl)\n\n\n\n\n\n  \n  \n    \n      predictor\n      .metric\n      prueba\n    \n  \n  \n    _óptimo\nrmse\n49\n  \n  \n  \n\n\n\n\nEste es nuestro error de prueba. Como la muestra de prueba no es muy grande, podríamos usar un método estándar para estimar su precisión, por ejemplo con bootstrap.\n\n\nEjemplo: regla\nAhora probemos con otro predictor, por ejemplo, supongamos que estamos usando la regla de “cada año de escolaridad aumenta ingresos potenciales en 20 unidades”, un predictor construido con reglas manuales que es\n\nf_regla <- function(x){\n  20 * x\n}\n\nAbajo lo graficamos en comparación con el modelo óptimo:\n\naños_x <- tibble(x = seq(0, 17, by = 0.5))\npreds_regla_tbl <- años_x |> \n  mutate(.pred = f_regla(x), predictor = \"regla\")\npreds_graf_tbl <- bind_rows(preds_regla_tbl, preds_graf_tbl)\nggplot(datos_tbl, aes(x = x)) +\n  geom_point(aes(y = y), colour = \"red\", alpha = 0.2) +\n  geom_line(data = preds_graf_tbl, aes(y = .pred, colour = predictor), size = 1.1) \n\n\n\n\n\neval_tbl <- prueba_tbl |>  \n  left_join(preds_graf_tbl, by = \"x\") \nresumen_tbl <- eval_tbl |>  \n  group_by(predictor, tipo) |> \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(resumen_tbl)\n\n\n\n\n\n  \n  \n    \n      predictor\n      .metric\n      prueba\n    \n  \n  \n    _óptimo\nrmse\n49\n    regla\nrmse\n91\n  \n  \n  \n\n\n\n\nObserva que el error es considerablemente mayor que el error que obtuvimos con el predictor óptimo del ejemplo anterior. Quisiéramos buscar algoritmos que tengan mejor desempeño aprendiendo de datos anteriores."
  },
  {
    "objectID": "02-principios-supervisado.html#aprendizaje",
    "href": "02-principios-supervisado.html#aprendizaje",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.3 Aprendizaje supervisado",
    "text": "2.3 Aprendizaje supervisado\nEn aprendizaje supervisado, buscamos construir la función \\(f\\) de manera automática usando datos. Supongamos entonces que tenemos un conjunto de datos etiquetados (sabemos la \\(y\\) correspondiente a cada \\(x\\)):\n\\[{\\mathcal L}=\\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}\\]\nque llamamos conjunto de entrenamiento.\nUn algoritmo de aprendizaje (aprender de los datos automáticamente) es una regla que asigna a cada conjunto de entrenamiento \\({\\mathcal L}\\) una función \\(\\hat{f}\\):\n\\[{\\mathcal L} \\to \\hat{f} = f_{\\mathcal L} \\]\nUna vez que construimos la función \\(\\hat{f}\\), podemos hacer predicciones.\nEl desempeño del predictor particular \\(\\hat{f}\\) se mide igual que antes: observamos otra muestra \\({\\mathcal T}\\), que llamamos muestra de prueba,\n\\[{\\mathcal T} = \\{(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}), (\\mathbf{x}^{(2)}, \\mathbf{y}^{(2)}), \\ldots, (\\mathbf{x}^{(m)}, \\mathbf{y}^{(m)})\\},\\]\ny calculamos el error de prueba. Si suponemos que \\(m\\) es suficientemente grande:\n\\[ \\widehat{Err}(\\hat{f}) = \\frac{1}{m} \\sum_{i=1}^m L(\\mathbf{y}^{(i)} , \\hat{f}(\\mathbf{x}^{(i)})) \\]\nes una buena aproximación del error de predicción \\(Err(\\hat{f})\\).\nAdicionalmente, definimos otra cantidad de menor interés, el error de entrenamiento, como\n\\[\\overline{err} = \\frac{1}{N}\\sum_{i=1}^N L(y^{(i)} , \\hat{f}(x^{(i)})).\\]\nque es una medida de qué tan bien se ajusta a \\(\\hat{f}\\) a los datos con los que se entrenó \\(\\hat{f}\\). Usualmente esta cantidad no es apropiada para medir el desempeño de un predictor, pues el algoritmo \\(\\hat{f}\\) incluye las “respuestas” \\(y_i\\) en su construcción, de forma que tiende a ser una estimación optimista del error de predicción.\n\nEjemplo: vecinos más cercanos\nConsideremos usar un método de \\(k\\)-vecinos más cercanos para resolver este problema. Este método es simple: si queremos hacer una predicción en las entradas \\(x\\), buscamos los puntos de entrenamiento con entradas \\(x^{(i)}\\) más cercanas a \\(x\\), que denotamos como \\(N_k(x)\\). Tomamos las \\(y\\) correspondientes a estas \\(x\\) y las usamos para hacer nuestra predicción:\n\\[f_2(x) = \\frac{1}{k}\\sum_{x^{(i)} \\in N_k(x)} y^{(i)}\\]\nPrimero obtendremos una muestra de entrenamiento:\n\nset.seed(12)\nentrena_tbl <- genera_datos(n = 20, tipo = \"entrena\")\n\nEn nuestro ejemplo, en lugar de usar un número fijo de vecinos, utilizaremos 10% de los datos más cercanos al punto donde queremos predecir:\n\n# modelo\nmodelo_kvecinos <- nearest_neighbor(neighbors = nrow(entrena_tbl) * 0.1, \n                                    weight_func = \"gaussian\") |> \n  set_mode(\"regression\") |> \n  set_engine(\"kknn\")\n# preprocesamiento\nreceta <- recipe(y ~ x, data = entrena_tbl |> select(x, y))\n# flujo\nflujo <- workflow() |> \n  add_recipe(receta) |> \n  add_model(modelo_kvecinos)\n# Ajustamos flujo\nflujo_ajustado_vecinos <- fit(flujo, entrena_tbl)\n\nHacemos predicciones y calculamos el error:\n\neval_tbl <- bind_rows(prueba_tbl, entrena_tbl) \nresumen_vmc_tbl <- \n  predict(flujo_ajustado_vecinos, eval_tbl) |> \n  mutate(predictor = \"vecinos\") |> \n  bind_cols(eval_tbl) |> \n  group_by(predictor, tipo) |> \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(resumen_vmc_tbl)\n\n\n\n\n\n  \n  \n    \n      predictor\n      .metric\n      entrena\n      prueba\n    \n  \n  \n    vecinos\nrmse\n36\n65\n  \n  \n  \n\n\n\n\nEl error de prueba, que es el que nos interesa hacer chico, es considerablemente grande. Si graficamos podemos ver el problema:\n\npreds_vmc <- predict(flujo_ajustado_vecinos, años_x) |> \n  bind_cols(años_x) |> mutate(predictor = \"vecinos\")\npreds_graf_tbl <- bind_rows(preds_vmc, preds_graf_tbl |> filter(predictor == \"_óptimo\"))\ng_1 <- ggplot(entrena_tbl, aes(x = x)) +\n  geom_line(data = preds_graf_tbl |> filter(predictor != \"regla\"), \n            aes(y = .pred, colour = predictor), size = 1.1) +\n  geom_point(aes(y = y), colour = \"red\") +\n  labs(subtitle = \"Óptimo vs ajustado\")\ng_1\n\n\n\n\nDonde vemos que este método intenta interpolar los datos, capturando ruido y produciendo variaciones que lo alejan del modelo óptimo. Esto lo notamos en lo siguiente:\n\nHay una brecha grande entre el error de entrenamiento y el error predictivo.\nEsta estimación de vecinos más cercanos es muy dependiente de la muestra de entrenamiento que obtengamos, pues intenta casi interpolar los datos. Esto sugiere alta variabilidad de las predicciones dependiendo de la muestra particular de entrenamiento que utilizamos.\nDecimos que este predictor está sobreajustado.\n\n\n\nEjemplo: regresión lineal\nAhora intentaremos con un modelo lineal. En este caso, utilizamos un predictor de la forma\n\\[f(x) = \\beta_0 + \\beta_1x\\]\nUsamos la muestra de entrenamiento para encontrar la \\(\\beta_0\\) y \\(\\beta_1\\) que minimizar el error sobre los datos disponibles de entrenamiento, lo cual es un problema de optimización relativamente fácil. Usamos entonces\n\\[\\hat{f}(x) =\\hat{\\beta}_0 + \\hat{\\beta}_1 x\\]\npara hacer nuestras predicciones.\n\nmodelo_lineal <- linear_reg() |> \n  set_mode(\"regression\") |> \n  set_engine(\"lm\")\nflujo_lineal <- workflow() |> \n  add_recipe(receta) |> \n  add_model(modelo_lineal)\n# Ajustamos\nflujo_ajustado_lineal <- fit(flujo_lineal, entrena_tbl)\n\nHacemos predicciones y calculamos el error:\n\neval_tbl <- bind_rows(prueba_tbl, entrena_tbl) \nresumen_lineal_tbl <- \n  predict(flujo_ajustado_lineal, eval_tbl) |> \n  mutate(predictor = \"lineal\") |> \n  bind_cols(eval_tbl) |> \n  group_by(predictor, tipo) |> \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(bind_rows(resumen_vmc_tbl, resumen_lineal_tbl))\n\n\n\n\n\n  \n  \n    \n      predictor\n      .metric\n      entrena\n      prueba\n    \n  \n  \n    vecinos\nrmse\n36\n65\n    lineal\nrmse\n49\n56\n  \n  \n  \n\n\n\n\nY el desempeño de este método es mejor que vecinos más cercanos (ver columna de prueba).\n\npreds_1 <- predict(flujo_ajustado_lineal, tibble(x = 0:17)) |> \n  bind_cols(tibble(x = 0:17, predictor = \"lineal\"))\npreds_graf_tbl <- bind_rows(preds_1, preds_graf_tbl)\ng_1 <- ggplot(datos_tbl, aes(x = x)) +\n  geom_point(aes(y = y), colour = \"red\", alpha = 0.1) +\n  geom_line(data = preds_graf_tbl |> filter(predictor %in% c(\"_óptimo\", \"lineal\")), \n            aes(y = .pred, colour = predictor), size = 1.1) +\n  labs(subtitle = \"Óptimo vs ajustado\")\ng_1\n\n\n\n\nEn este caso:\n\nNo hay brecha tan grande entre el error de entrenamiento y el error predictivo\nObservamos patrones claros de desajuste: el predictor lineal no captura el patrón curvo que presentan los datos: en la parte media de las \\(x\\) tiende a producir predicciones demasiado altas y lo contario ocurre en los extremos\nDecimos que esté modelo presenta subajuste."
  },
  {
    "objectID": "02-principios-supervisado.html#entendiendo-el-error-de-predicción",
    "href": "02-principios-supervisado.html#entendiendo-el-error-de-predicción",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.4 Entendiendo el error de predicción",
    "text": "2.4 Entendiendo el error de predicción\nEstos dos ejemplos de predictores tienen mal desempeño (comparado con el óptimo por distintas razones. Para entender qué pasa, consideramos los residuales de cada ajuste, para un caso de prueba:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x})\\]\nEsta cantidad puede tener un valor positivo o negativo grande, lo que indica errores grandes. Sea \\(f^*\\) el predictor óptimo que explicamos arriba. Entonces, en primer lugar:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) = \\underbrace{(f^* (\\mathbf{x}) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}))}_\\text{reducible} + \\underbrace{(\\mathbf{y}- f^*(\\mathbf{x}))}_\\text{irreducible}.\\]\ndonde vemos que si las dos cantidades de la derecha están cercanas a cero, entonces el residual es cercano a cero (la predicción es precisa):\n\nError irreducible: no depende de nuestro algoritmo, sino de la información que tenemos en \\(x\\) para predecir \\(y\\). Si queremos hacer esté error más chico, necesitamos incluir otras variables \\(x\\) relevantes para predecir \\(y\\).\nError reducible: qué tan lejos nuestro método está del óptimo. Podemos mejorar este error seleccionando nuestra muestra de entrenamiento y método de predicción \\(\\hat{f}\\) de manera adecuada.\n\nEn nuestros dos ejemplos anteriores, el error reducible era considerablemente grande (como podemos verificar comparando con el predictor óptimo, que sólo sufre de error irreducible). Pero la razón por la que ese error reducible es grande es diferente en cada caso.\nPara explicar la diferencia, podemos considerar \\(f_{lim},\\) el predictor que obtendríamos con nuestro método si ajustáramos nuestro método con la población completa, de manera que \\(\\hat{f_{\\mathcal{L}}}\\to f_{\\lim}\\) cuando el tamaño de la muestra de entrenamiento \\({\\mathcal{L}}\\) se hace muy grande.\nPodemos refinar nuestra descomposición y escribir:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) = \\underbrace{f^* (\\mathbf{x}) - f_{\\lim}(\\mathbf{x})}_\\text{sesgo} +\n  \\underbrace{f_{\\lim}(\\mathbf{x}) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x})}_\\text{variabilidad} +\n  \\underbrace{y - f^*(\\mathbf{x})}_\\text{irreducible}.\\]\nEl error reducible ahora se descompone en dos partes:\n\nEl sesgo o subajuste: que se debe a la incapacidad de nuestro modelo de capturar la forma del predictor óptimo, incluso conociendo toda la población. Este término no depende de la muestra de entrenamiento: depende de la capacidad de nuestro método para aprender en condiciones ideales.\nLa variabilidad o error de estimación o sobreajuste: este error resulta de que tenemos información limitada de la población, y nuestro ajuste se aleja de lo que obtendríamos con información completa. Esta parte del error varía dependiendo de la muestra particular de entrenamiento que utilizamos.\n\nEn nuestros dos ejemplos, intuímos que vecinos más cercanos sufre más de variabilidad o sobreajuste y regresión lineal de sesgo. Podemos verificar viendo qué pasa con los dos métodos cuando los entrenamos con la población completa.\nObservación: Generalmente el término variabilidad se refiere a la variación de un estimador alrededor de su valor esperado, cuando observamos sus distintas posibles estimadociones a los largo de varias muestras de entrenamiento. Podemos descomponer el segundo término de arriba como:\n\\[ f_{\\lim}(\\mathbf{x}) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) = f_{\\lim}(\\mathbf{x}) - E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})) + E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x})\\]\ndonde el valor esperado es sobre todas las muestras de entrenamiento de un tamaño fijo \\(n\\) que podríamos obtener. El segundo término puede llamarse variabilidad, mientras que el primero es el sesgo que obtenemos al usar una muestra \\(n\\) finita.\nEn esta versión, una parte del sesgo puede deberse a que nuestro método no tiene capacidad suficiente para encontrar la forma correcta de \\(f^*(x)\\), incluso con datos completos (el primer término del sesgo), mientras que el segundo término del sesgo indica que con el tamaño de muestra \\(n\\) que tenemos, su capacidad no se puede alcanzar."
  },
  {
    "objectID": "02-principios-supervisado.html#ejemplo-fuentes-de-error",
    "href": "02-principios-supervisado.html#ejemplo-fuentes-de-error",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.5 Ejemplo: fuentes de error",
    "text": "2.5 Ejemplo: fuentes de error\nVamos a ver qué sucede con nuestros dos métodos si utilizamos una muestra grande:\n\nmuestra_grande_tbl <- sample_n(poblacion_tbl, 10000) |> \n  mutate(tipo = \"entrena\")\nmodelo_kvecinos <- nearest_neighbor(neighbors = nrow(muestra_grande_tbl) * 0.10, \n                                    weight_func = \"gaussian\") |> \n  set_mode(\"regression\") |> \n  set_engine(\"kknn\")\n# Ajustamos (no es necesario usar la población completa para este ejemplo)\nflujo_vecinos <- workflow() |> \n  add_recipe(receta) |> \n  add_model(modelo_kvecinos)\nflujo_ajustado_vecinos_limite <- fit(flujo_vecinos, muestra_grande_tbl)\nflujo_ajustado_lineal_limite <- fit(flujo_lineal, muestra_grande_tbl)\n\neval_tbl <- bind_rows(prueba_tbl, muestra_grande_tbl)\nresumen_vecinos_lim_tbl <- \n  predict(flujo_ajustado_vecinos_limite, eval_tbl) |> \n  mutate(predictor = \"vecinos_limite\") |> \n  bind_cols(eval_tbl) |> \n  group_by(predictor, tipo) |> \n  rmse(truth = y, estimate = .pred) \nresumen_lineal_lim_tbl <- \n  predict(flujo_ajustado_lineal_limite, eval_tbl) |> \n  mutate(predictor = \"lineal_limite\") |> \n  bind_cols(eval_tbl) |> \n  group_by(predictor, tipo) |> \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(bind_rows(resumen_vmc_tbl, resumen_lineal_tbl, resumen_vecinos_lim_tbl, resumen_lineal_lim_tbl) |> arrange(predictor))\n\n\n\n\n\n  \n  \n    \n      predictor\n      .metric\n      entrena\n      prueba\n    \n  \n  \n    lineal\nrmse\n49\n56\n    lineal_limite\nrmse\n54\n54\n    vecinos\nrmse\n36\n65\n    vecinos_limite\nrmse\n49\n49\n  \n  \n  \n\n\n\n\n¿Qué patrones ves en esta tabla? Podemos también graficar para entender mejor qué está pasando:\n\npreds_1 <- predict(flujo_ajustado_vecinos_limite, tibble(x = 0:17)) |> \n  bind_cols(tibble(x = 0:17, predictor = \"vecinos_limite\"))\npreds_2 <- predict(flujo_ajustado_lineal_limite, tibble(x = 0:17)) |> \n  bind_cols(tibble(x = 0:17, predictor = \"lineal_limite\"))\npreds_graf_tbl <- bind_rows(preds_1, preds_2, preds_graf_tbl) |> \n  mutate(predictor = factor(predictor))\ng_1 <- ggplot(datos_tbl, aes(x = x)) +\n  geom_line(data = preds_graf_tbl |> \n            filter(str_detect(predictor, \"vecinos|_óptimo\")), \n            aes(y = .pred, colour = predictor), size = 1.1) +\n  labs(subtitle = \"Vecinos\") \ng_2 <- ggplot(datos_tbl, aes(x = x)) +\n  geom_line(data = preds_graf_tbl |> \n            filter(str_detect(predictor, \"lineal|_óptimo\")), \n            aes(y = .pred, colour = predictor), size = 1.1) +\n  labs(subtitle = \"Lineal\") \ng_1 + g_2 \n\n\n\n\nEsto quiere decir que:\n\nNuestro método de vecinos más cercanos no tiene errores por sesgo, sino más bien por sobreajuste o variabilidad.\nNuestro método lineal no tiene mucha variabilidad (el estimado con una muestra grande es casi igual al de la muestra de entrenamiento), sino más bien por sesgo\nEl error por sesgo se reduce usando métodos más flexibles o menos restringidos que puedan capturar patrones claros en los datos.\nPara reducir la variabilidad podemos usar métodos más simples o restringidos que no capturen tanto ruido. Aumentar la muestra de entrenamiento puede ayudar también.\nEl error irreducible se puede reducir incorporando información adicional relevante a las entradas.\n\n¿Qué cosas aumentan o disminuyen la complejidad de nuestro predictor?\n\nQuitar variables reduce la complejidad, pero potencialmente aumenta el sesgo y el error irreducible.\nAgregar variables derivadas puede reducir el sesgo y el error irreducible, pero también puede incrementar la variabilidad o el sobreajuste.\nRestringir el ajuste de alguna forma o penalizar predictores poco creíbles reduce la variabilidad, pero aumenta el sesgo.\nUsar métodos más complicados en general o con más parámetros puede reducir el sesgo, pero quizá también aumenta el sobreajuste."
  },
  {
    "objectID": "02-principios-supervisado.html#agregando-más-información-y-error-irreducible",
    "href": "02-principios-supervisado.html#agregando-más-información-y-error-irreducible",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.6 Agregando más información y error irreducible",
    "text": "2.6 Agregando más información y error irreducible\nPodemos ver qué sucede cuando tenemos disponibles más variables relevantes. En este caso, probaremos con dos entradas:\n\ngenera_datos_2 <- function(n = 500, tipo = NULL){\n  dat_tbl <- tibble(nse = runif(n, 0, 100)) |>\n    mutate(estudio_años = floor(rnorm(n, 1.5 * sqrt(nse), 1))) |>\n    mutate(estudio_años = pmax(0, pmin(17, estudio_años))) |> \n    mutate(habilidad = rnorm(n, 100 + 0.1 * nse, 1)) |> \n    mutate(z = 100 + (habilidad/100) * ( 20 * nse + 5 * (estudio_años))) |> \n    mutate(ingreso = pmax(0, 0.2*(z + rnorm(n, 0, 150))))\n  obs_tbl <- dat_tbl |> \n    mutate(tipo = tipo, id = 1:n)\n  obs_tbl |> select(id, tipo, x_1 = estudio_años, x_2 = nse,  y = ingreso)\n}\n\n\nentrena_tbl <- genera_datos_2(20, tipo = \"entrena\")\nprueba_tbl <- genera_datos_2(500, tipo = \"prueba\")\nreceta_2 <- recipe(y ~ x_1 + x_2, data = entrena_tbl)\nflujo_lineal <- workflow() |> \n  add_recipe(receta_2) |> \n  add_model(modelo_lineal)\n# Ajustamos\nflujo_ajustado_lineal <- fit(flujo_lineal, entrena_tbl)\npredict(flujo_ajustado_lineal, bind_rows(entrena_tbl, prueba_tbl)) |> \n  bind_cols(bind_rows(entrena_tbl, prueba_tbl)) |>\n  group_by(tipo) |> \n  rmse(truth = y, estimate = .pred) |> gt() |> \n  fmt_number(.estimate, decimals = 1)\n\n\n\n\n\n  \n  \n    \n      tipo\n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    entrena\nrmse\nstandard\n27.1\n    prueba\nrmse\nstandard\n31.5\n  \n  \n  \n\n\n\n\nY vemos cómo inmediatamente redujimos el error de predicción: en este caso, aunque la variabilidad aumentó un poco (tenemos más parámetros que estimar vs el modelo con una sola variable), la reducción en el sesgo y en el error irreducible es tan grande que el desempeño es muy superior. Examina el caso de vecinos más cercanos."
  },
  {
    "objectID": "02-principios-supervisado.html#acerca-de-la-estimación-del-error-de-predicción",
    "href": "02-principios-supervisado.html#acerca-de-la-estimación-del-error-de-predicción",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.7 Acerca de la estimación del error de predicción",
    "text": "2.7 Acerca de la estimación del error de predicción\nCuando usamos una muestra de prueba limitada, podemos evaluar la precisión de nuestra estimación del error de predicción usando por ejemplo el bootstrap. En nuestro ejemplo anterior podríamos hacer los siguiente:\n\nlibrary(infer)\npreds <- predict(flujo_ajustado_lineal, bind_rows(prueba_tbl)) |> \n  bind_cols(prueba_tbl) \npreds |> \n  generate(reps = 1000, type = \"bootstrap\", variables = id) |> \n  group_by(replicate, tipo) |> \n  rmse(truth = y, estimate = .pred) |> \n  select(replicate, tipo, stat = .estimate) |>\n  get_ci(level = 0.90) |> \n  gt() |> fmt_number(where(is_double), decimals = 1)\n\nWarning: The `variables` argument is only relevant for the \"permute\" generation\ntype and will be ignored.\n\n\n\n\n\n\n  \n  \n    \n      lower_ci\n      upper_ci\n    \n  \n  \n    29.9\n33.0"
  },
  {
    "objectID": "02-principios-supervisado.html#resumen",
    "href": "02-principios-supervisado.html#resumen",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.8 Resumen",
    "text": "2.8 Resumen\n\n\n\n\n\n\nTarea fundamental del análisis supervisado\n\n\n\n\nUsando datos de entrenamiento \\({\\mathcal L}\\), construimos una funcion \\(\\hat{f}\\) para predecir. Estas funciones se ajustan usualmente intentando estimar directamente el predictor óptimo \\(f^*(x)\\) (si lo conocemos teóricamente), o indirectamente intentando minimizar la pérdida sobre el conjunto de entrenamiento.\nSi observamos nuevos valores \\(\\mathbf{x}\\), nuestra predicción es \\(\\hat{y} = \\hat{f}(\\mathbf{x})\\).\nBuscamos que cuando observemos nuevos casos para predecir, nuestro error de predicción sea bajo en promedio (\\(Err\\) sea bajo).\nUsualmente estimamos \\(Err\\) mediante una muestra de prueba o validación \\({\\mathcal T}\\).\nNos interesan métodos de construir \\(\\hat{f}\\) que produzcan errores de predicción bajos.\n\n\n\n\nNótese que el error de entrenamiento se calcula sobre la muestra \\({\\mathcal L}\\) que se usó para construir \\(\\hat{f}\\), mientras que el error de predicción se estima usando una muestra independiente \\({\\mathcal T}\\).\n\\(\\hat{Err}\\) es una estimación razonable de el error de predicción \\(Err\\) (por ejemplo, \\(\\hat{Err} \\to Err\\) cuando el tamaño de la muestra de prueba crece), pero \\(\\overline{err}\\) típicamente es una estimación mala del error de predicción.\n\n\n\n\n\n\n\nReduciendo el error de predicción\n\n\n\nPara reducir el error de predicción, podemos:\n\nIncluir variables relevantes que reduzcan el error irreducible\nReducir variabilidad usando métodos más estables o menos complejos\nReducir sesgo usando métodos más flexibles\nUsar métodos con la estructura adecuada para el problema\n\nGeneralmente 2 y 3 están en contraposición, a lo que muchas veces se le llama equilibrio de varianza y sesgo. Los puntos 1 y 4 generalmente mejoran los resultados reduciendo tanto sesgo como variabilidad."
  },
  {
    "objectID": "02-principios-supervisado.html#qué-cosas-no-veremos-en-este-curso",
    "href": "02-principios-supervisado.html#qué-cosas-no-veremos-en-este-curso",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.9 Qué cosas no veremos en este curso",
    "text": "2.9 Qué cosas no veremos en este curso\nEn este curso nos concentraremos en la construcción, evaluación y mejora de modelos predictivos. Para que estas ideas funcionen en problemas reales, hay más aspectos a considerar que no discutiremos con detalle (y muchas veces son considerablemente más difíciles de la teoría y los algoritmos):\n\nPara entender exactamente cuál es el problema que queremos resolver se requiere trabajo analítico considerable, y también trabajo en entender aspectos del área o negocio donde nos interesa usar aprendizaje máquina. Muchas veces es fácil resolver un problema muy preciso, que tenemos a la mano, pero que más adelante nos damos cuenta de que no es útil.\nEstos dos puntos incluyen indentificar las métricas que queremos mejorar, lo cual no siempre se claro. Optimizar métricas incorrectas es poco útil en el mejor de los casos, y en los peores pueden causar daños. Evitar esto requiere monitoreo constante de varios aspectos del funcionamiento de nuestros modelos y sus consecuencias.\n¿Cómo poner en producción modelos y mantenerlos? Un flujo apropiado de trabajo, y de entrenamiento continuo puede ser la diferencia entre entre un modelo exitoso o uno que se vuelve fuente de dificultades y confusión."
  },
  {
    "objectID": "03-metodos-locales.html",
    "href": "03-metodos-locales.html",
    "title": "3  Métodos locales no estructurados",
    "section": "",
    "text": "De la discusión de la sección anterior, y examinando el método de \\(k\\) vecinos más cercanos, puede dar la impresión de que si tenemos suficientes datos, métodos locales como \\(k\\) vecinos pueden ser superiores a otros métodos más estructurados como regresión lineal, que necesariamente incurren en sesgo porque su estructura siempre está mal especificada.\nSin embargo, no es necesario que se cumpla exactamente el supuesto lineal para que los predictores lineales funcionen, y veremos que en casos típicos los métodos locales como \\(k\\)-vecinos más cercanos rara vez funcionan apropiadamente."
  },
  {
    "objectID": "03-metodos-locales.html#controlando-complejidad",
    "href": "03-metodos-locales.html#controlando-complejidad",
    "title": "3  Métodos locales no estructurados",
    "section": "3.1 Controlando complejidad",
    "text": "3.1 Controlando complejidad\nPrimero examinamos cómo controlamos el nivel de complejidad para un método local como \\(k\\) vecinos más cercanos. La idea es que:\n\nMás complejidad: Si tomamos \\(k\\) demasiado chica, cada estimación usa pocos datos y puede ser ruidosa (incurrimos en variabilidad)\nMenos complejidad: Si tomamos \\(k\\) demasiado grande, cada estimación usa potencialmente datos no relevantes muy lejanos a donde queremos predecir (incurrimos en sesgo)\n\nComenzamos con un ejemplo simple en dimensión baja:\n\nEjemplo\n\nlibrary(tidyverse)\nlibrary(gt)\nauto <- read_csv(\"../datos/auto.csv\")\ndatos <- auto[, c('name', 'weight','year', 'mpg', 'displacement')]\ndatos <- datos %>% mutate(\n  peso_kg = weight * 0.45359237,\n  rendimiento_kpl = mpg * (1.609344 / 3.78541178), \n  año = year)\n\nVamos a separa en muestra de entrenamiento y de prueba estos datos. Podemos hacerlo como sigue (75% para entrenamiento aproximadamente en este caso, así obtenemos alrededor de 100 casos para prueba):\n\nlibrary(tidymodels)\nset.seed(121)\ndatos_split <- initial_split(datos, prop = 0.75)\ndatos_entrena <- training(datos_split)\ndatos_prueba <- testing(datos_split)\nnrow(datos_entrena)\n\n[1] 294\n\nnrow(datos_prueba)\n\n[1] 98\n\n\nVamos a usar año y peso de los coches para predecir su rendimiento:\n\nggplot(datos_entrena, aes(x = peso_kg, y = rendimiento_kpl, colour = año)) +\n  geom_point()\n\n\n\n\nProbaremos con varios valores para \\(k\\), el número de vecinos más cercanos. La función de predicción ajustada es entonces:\n\n# nótese que normalizamos entradas - esto también es importante\n# hacer cuando hacemos vecinos más cercanos, pues en otro caso\n# las variables con escalas más grandes dominan el cálculo\nvmc_1 <- nearest_neighbor(neighbors = tune(), weight_func = \"gaussian\") |>  \n  set_engine(\"kknn\") |>  \n  set_mode(\"regression\")\nreceta_vmc <- recipe(rendimiento_kpl ~ peso_kg + año, datos_entrena) |> \n  step_normalize(all_predictors()) \nflujo_vecinos <- workflow() |>  \n  add_recipe(receta_vmc) |> \n  add_model(vmc_1)\nvecinos_params <- parameters(neighbors(range = c(1, 100)))\nvecinos_grid <- grid_regular(vecinos_params, levels = 100)\n\n\nmis_metricas <- metric_set(rmse)\nr_split <- manual_rset(list(datos_split), \"validación\")\nvecinos_eval_tbl <- tune_grid(flujo_vecinos,\n                            resamples = r_split,\n                            grid = vecinos_grid,\n                            metrics = mis_metricas) \nvecinos_ajustes_tbl <- vecinos_eval_tbl %>%\n  unnest(cols = c(.metrics)) %>% \n  select(id, neighbors, .metric, .estimate)\nggplot(vecinos_ajustes_tbl, aes(x = neighbors, y = .estimate)) +\n  geom_line() + geom_point()\n\n\n\n\nDonde obtenemos más o menos lo que esperaríamos: modelos con muy pocos vecinos o demasiados vecinos se desempeñan relativamente mal. Podemos visualizar nuestras predicciones y los datos de entrenamiento de la siguiente forma:\n\nmejor_rmse <- select_best(vecinos_eval_tbl, metric = \"rmse\")\najuste_1 <- finalize_workflow(flujo_vecinos, mejor_rmse) |> \n  fit(datos_entrena)\ndat_graf <- tibble(peso_kg = seq(900, 2200, by = 10)) %>% \n  crossing(tibble(año = c(70, 75, 80)))\ndat_graf <- dat_graf %>% \n  mutate(pred_1 = predict(ajuste_1, dat_graf) %>% pull(.pred))\nggplot(datos_entrena, aes(x = peso_kg, group = año, colour = año)) +\n  geom_point(aes(y = rendimiento_kpl), alpha = 0.6) + \n  geom_line(data = dat_graf, aes(y = pred_1),  size = 1.2)\n\n\n\n\nEl método parece funcionar razonablemente bien para este problema simple. Sin embargo, si el espacio de entradas no es de dimensión baja, entonces podemos encontrarnos con dificultades."
  },
  {
    "objectID": "03-metodos-locales.html#la-maldición-de-la-dimensionalidad",
    "href": "03-metodos-locales.html#la-maldición-de-la-dimensionalidad",
    "title": "3  Métodos locales no estructurados",
    "section": "3.2 La maldición de la dimensionalidad",
    "text": "3.2 La maldición de la dimensionalidad\nEl método de k-vecinos más cercanos funciona mejor cuando\n\nNo es necesario hacer \\(k\\) demasiado grande, de forma que terminemos tomando valores lejanos que inducen sesgo.\nNo es necesario hacer \\(k\\) demasiado chica, de forma que nuestras predicciones sean inestables.\n\n\n\n\n\n\n\nMaldición de la dimensionalidad\n\n\n\nEn dimensión alta, para la mayoría de las \\(\\mathbf{x}\\) donde queremos hacer predicciones típicamente no existen vecinos cercanos, aún para conjuntos de entrenamiento muy grandes.\n\n\nEsto implica que incluso con \\(k\\) chica (que produce sobreajuste) sufrimos simultáneamente de sesgo (por extrapolación a regiones no relevantes). Esto implica que en estos casos (que son muy comunes) métodos locales no funcionan bien independientemente de cómo afinemos el algoritmo.\n\nEjemplo\nConsideremos que la salida Y es determinística \\(Y = e^{-8\\sum_{j=1}^p x_j^2}\\). Vamos a usar 1-vecino más cercano para hacer predicciones, con una muestra de entrenamiento de 1000 casos. Generamos $x^{i}’s uniformes en \\([ 1,1]\\), para \\(p = 2\\), y calculamos la respuesta \\(Y\\) para cada caso:\n\nfun_exp <- function(x) exp(-8 * sum(x ^ 2))\nx <- map(1:1000, ~ runif(2, -1, 1))\ndat <- tibble(x = x) %>% \n        mutate(y = map_dbl(x, fun_exp))\nggplot(dat %>% mutate(x_1 = map_dbl(x, 1), x_2 = map_dbl(x, 2)), \n       aes(x = x_1, y = x_2, colour = y)) + geom_point()\n\n\n\n\nLa mejor predicción en \\(x_0 = (0,0)\\) es \\(f((0,0)) = 1\\). El vecino más cercano al origen es\n\ndat <- dat %>% mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) %>% \n  arrange(dist_origen)\nmas_cercano <- dat[1, ]\nmas_cercano\n\n# A tibble: 1 × 3\n  x             y dist_origen\n  <list>    <dbl>       <dbl>\n1 <dbl [2]> 0.995      0.0261\n\nmas_cercano$x[[1]]\n\n[1] -0.025090354  0.007277334\n\n\nNuestra predicción es entonces \\(\\hat{f}(0)=\\) 0.994555, que es bastante cercano al valor verdadero (1).\nAhora intentamos hacer lo mismo para dimensión \\(p=8\\).\n\nx <- map(1:1000, ~ runif(8, -1, 1))\ndat <- tibble(x = x) %>% \n       mutate(y = map_dbl(x, fun_exp))\ndat <- dat %>% mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) %>% \n  arrange(dist_origen)\nmas_cercano <- dat[1, ]\nmas_cercano\n\n# A tibble: 1 × 3\n  x             y dist_origen\n  <list>    <dbl>       <dbl>\n1 <dbl [8]> 0.104       0.532\n\nmas_cercano$x[[1]]\n\n[1]  0.30027994  0.36774993 -0.06613864 -0.03673154  0.12260975  0.16718980\n[7] -0.01866598 -0.09308947\n\n\nY el resultado es un desastre. Nuestra predicción es\n\nmas_cercano$y\n\n[1] 0.1038249\n\n\nNecesitariamos una muestra de alrededor de un millón de casos para obtener resultados no tan malos (haz pruebas).\n¿Qué es lo que está pasando? La razón es que en dimensiones altas, los puntos de la muestra de entrenamiento están muy lejos unos de otros, y están cerca de la frontera, incluso para tamaños de muestra relativamente grandes como n = 1000. Cuando la dimensión crece, la situación empeora exponencialmente."
  },
  {
    "objectID": "03-metodos-locales.html#regresión-lineal-en-dimensión-alta",
    "href": "03-metodos-locales.html#regresión-lineal-en-dimensión-alta",
    "title": "3  Métodos locales no estructurados",
    "section": "3.3 Regresión lineal en dimensión alta",
    "text": "3.3 Regresión lineal en dimensión alta\nAhora intentamos algo similar con una función que es razonable aproximar con una función lineal:\n\nfun_cuad <- function(x)  0.5 * (1 + x[1])^2\n\nY queremos predecir para \\(x=(0,0,\\ldots,0)\\), cuyo valor exacto es\n\nfun_cuad(0)\n\n[1] 0.5\n\n\nLos datos se generan de la siguiente forma:\n\nsimular_datos <- function(p = 40){\n    x <- map(1:1000,  ~ runif(p, -1, 1))\n    dat <- tibble(x = x) %>% mutate(y = map_dbl(x, fun_cuad)) \n    dat\n}\n\nPor ejemplo para dimensión baja \\(p=1\\) (nótese que una aproximación lineal es razonable):\n\nejemplo <- simular_datos(p = 1) %>% mutate(x = unlist(x))\nggplot(ejemplo, aes(x = x, y = y)) + geom_point() +\n    geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nAhora repetimos el proceso en dimensión \\(p=40\\): simulamos las entradas, y aplicamos un vecino más cercano\n\nvmc_1 <- function(dat){\n    dat <- dat %>% \n        mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) %>% \n        arrange(dist_origen)\n        mas_cercano <- dat[1, ]\n        mas_cercano$y\n}\nset.seed(834)\ndat <- simular_datos(p = 40)\nvmc_1(dat)\n\n[1] 1.206478\n\n\nEste no es un resultado muy bueno. Sin embargo, regresión se desempeña considerablemente mejor:\n\nregresion_pred <- function(dat){\n    p <- length(dat$x[[1]])\n    dat_reg <- cbind(\n        y = dat$y, \n        x = matrix(unlist(dat$x), ncol = p, byrow=T)) %>% \n        as.data.frame()\n    mod_lineal <- lm(y ~ ., dat = dat_reg)\n    origen <- data.frame(matrix(rep(0, p), 1, p))\n    names(origen) <- names(dat_reg)[2:(p+1)]\n    predict(mod_lineal, newdata = origen)\n}\nregresion_pred(dat)\n\n        1 \n0.6677861 \n\n\nLa razón de este mejor desempeño de regresión es que en este caso, el modelo lineal explota la estructura aproximadamente lineal del problema (¿cuál estructura lineal? haz algunas gráficas). Nota: corre este ejemplo varias veces con semilla diferente.\nSolución: vamos a hacer varias simulaciones, para ver qué modelo se desempeña mejor.\n\nsims <- map(1:200, function(i){\n    dat <- simular_datos(p = 40)\n    vmc_y <- vmc_1(dat)\n    reg_y <- regresion_pred(dat)\n    tibble(rep = i, \n           error = c(abs(vmc_y - 0.5), abs(reg_y - 0.5)), \n            tipo = c(\"vmc\", \"regresion\"))\n}) %>% bind_rows\nggplot(sims, aes(x = tipo, y = error)) + geom_boxplot() \n\n\n\n\nAsí que típicamente el error de vecinos más cercanos es más alto que el de regresión. El error esperado es para vmc es más de doble que el de regresión:\n\nsims %>% group_by(tipo) %>% \n  summarise(media_error = mean(error)) |> \n  gt()\n\n\n\n\n\n  \n  \n    \n      tipo\n      media_error\n    \n  \n  \n    regresion\n0.1662124\n    vmc\n0.3542532\n  \n  \n  \n\n\n\n\nLo que sucede más específicamente es que en regresión lineal utilizamos todos los datos para hacer nuestra estimación en cada predicción. Si la estructura del problema es aproximadamente lineal, entonces regresión lineal explota la estructura para hacer pooling de toda la información para construir predicción con sesgo y varianza bajas. En contraste, vecinos más cercanos sufre de varianza alta.\n\n\n\n\n\n\nMétodos locales\n\n\n\nLos métodos localesa muchas veces no funcionan bien en dimensión alta. La razón es que:\n\nEl sesgo es alto, pues promediamos puntos muy lejanos al lugar donde queremos predecir (aunque tomemos pocos vecinos cercanos).\nEn el caso de que encontremos unos pocos puntos cercanos, la varianza también puede ser alta porque promediamos relativamente pocos vecinos.\n\nMétodos con más estructura global, apropiada para el problema, logran explotar apropiadamente información de puntos que no están tan cerca del lugar donde queremos predecir.\n\n\nMuchas veces el éxito en la predicción depende de establecer esas estructuras apropiadas (por ejemplo, efectos lineales cuando variables tienen efectos aproximadamente lineales, árboles cuando hay algunas interacciones, redes convolucionales para procesamiento de imágenes y señales, dependencia del contexto inmediato en modelos de lenguaje, etc.)"
  },
  {
    "objectID": "99-referencias.html",
    "href": "99-referencias.html",
    "title": "Referencias",
    "section": "",
    "text": "Bishop, Christopher M. 2006. Pattern Recognition and Machine\nLearning (Information Science and Statistics). Secaucus, NJ, USA:\nSpringer-Verlag New York, Inc.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep\nLearning. MIT Press.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. The\nElements of Statistical Learning. Springer Series in Statistics.\nSpringer New York Inc. http://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2014. An Introduction to Statistical Learning: With Applications in\nr. Springer Publishing Company, Incorporated. http://www-bcf.usc.edu/~gareth/ISL/.\n\n\nKuhn, M., and J. Silge. 2022. Tidy Modeling with r. O’Reilly\nMedia. https://books.google.com.mx/books?id=9cJ6EAAAQBAJ."
  }
]