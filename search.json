[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aprendizaje Máquina",
    "section": "",
    "text": "Todas las notas y material del curso estarán en este repositorio.\n\nIntroducción al aprendizaje máquina\nMétodos locales y regresión lineal\nIngeniería de variables de entrada\nRegularización\nProblemas de clasificación y regresión logística\nMétodos de remuestreo y validación cruzada\nRedes neuronales\nÁrboles, bosques aleatorios y boosting\nDiagnóstico y mejora en problemas de aprendizaje supervisado\nComponentes principales y análisis de conglomerados\n\n\n\n\nTareas semanales (20%)\nExamen parcial (30% práctico, 20% teórico)\nUn examen final (30% práctico)\n\n\n\n\nCada semestre las notas cambian, en algunas partes considerablemente. Las de este semestre están en este repositorio, incluyendo ejemplos, ejercicios y tareas.\n\n\n\n\nAn Introduction to Statistical Learning, James et al. (2014)\nDeep Learning, Goodfellow, Bengio, y Courville (2016)\nTidy Modeling with R, Kuhn y Silge (2022)\n\n\n\n\n\nPattern Recognition and Machine Learning, Bishop (2006)\nThe Elements of Statistical Learning, Hastie, Tibshirani, y Friedman (2017)\nPredicción conformehttps://people.eecs.berkeley.edu/~angelopoulos/blog/posts/gentle-intro/\n\n\n\n\nPara hacer las tareas y exámenes pueden usar cualquier lenguaje o flujo de trabajo que les convenga (R o Python, por ejemplo) - el único requisito esté basado en código y no point-and-click. En lo posible utilizamos librerías especializadas que se pueden utilizar desde varias plataformas (keras, por ejemplo).\n\nR Sitio de R (CRAN)\nRstudio Interfaz gráfica para trabajar en R.\nRecursos para aprender R\n\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Secaucus, NJ, USA: Springer-Verlag New York, Inc.\n\n\nGoodfellow, Ian, Yoshua Bengio, y Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nHastie, Trevor, Robert Tibshirani, y Jerome Friedman. 2017. The Elements of Statistical Learning. Springer Series en Statistics. Springer New York Inc. http://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, y Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Incorporated. http://www-bcf.usc.edu/~gareth/ISL/.\n\n\nKuhn, M., y J. Silge. 2022. Tidy Modeling with R. O’Reilly Media. https://books.google.com.mx/books?id=9cJ6EAAAQBAJ."
  },
  {
    "objectID": "01-introduccion.html",
    "href": "01-introduccion.html",
    "title": "1  Introducción",
    "section": "",
    "text": "Métodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión.\nEn este curso nos enfocamos en las tareas de aprendizaje supervisado (predecir o estimar una variable respuesta a partir de datos de entrada) y aprendizaje no supervisado (describir estructuras interesantes en datos, donde no necesariamente hay una respuesta que predecir). Existe también aprendizaje por refuerzo, en donde buscamos aprender a tomar decisiones en un entorno en donde la decisión afecta directa e inmediatamente al entorno.\n\n\n\nPredecir si un cliente de tarjeta de crédito va a caer en impago en los próximos tres meses.\nReconocer palabras escritas a mano (OCR).\nDetectar llamados de ballenas en grabaciones de boyas.\nEstimar el ingreso mensual de un hogar a partir de las características de la vivienda, posesiones y equipamiento y localización geográfica.\nDividir a los clientes de Netflix según sus gustos.\nRecomendar artículos a clientes de un programa de lealtad o servicio online.\n\nLas razones usuales para intentar resolver estos problemas computacionalmente son diversas:\n\nQuisiéramos obtener una respuesta barata, rápida, automatizada, y con suficiente precisión. Por ejemplo, reconocer caracteres en una placa de coche de una fotografía se puede hacer por personas, pero eso es lento y costoso. Igual oír cada segundo de grabación de las boyas para saber si hay ballenas o no. Hacer mediciones directas del ingreso de un hogar requiere mucho tiempo y esfuerzo.\nQuisiéramos superar el desempeño actual de los expertos o de reglas simples utilizando datos: por ejemplo, en la decisión de dar o no un préstamo a un solicitante, puede ser posible tomar mejores decisiones con algoritmos que con evaluaciones personales o con reglas simples que toman en cuenta el ingreso mensual, por ejemplo.\nAl resolver estos problemas computacionalmente tenemos oportunidad de aprender más del problema que nos interesa: estas soluciones forman parte de un ciclo de análisis de datos donde podemos aprender de una forma más concentrada cuáles son características y patrones importantes de nuestros datos.\n\nEs posible aproximarse a todos estos problemas usando reglas (por ejemplo, si los pixeles del centro de la imagen están vacíos, entonces es un cero, si el crédito total es mayor al 50% del ingreso anual, declinar el préstamo, etc). Las razones para no tomar un enfoque de reglas construidas “a mano”:\n\nCuando conjuntos de reglas creadas a mano se desempeñan mal (por ejemplo, para otorgar créditos, reconocer caracteres, etc.)\nReglas creadas a mano pueden ser difíciles de mantener (por ejemplo, un corrector ortográfico), pues para problemas interesantes muchas veces se requieren grandes cantidades de reglas. Por ejemplo: ¿qué búsquedas www se enfocan en dar direcciones como resultados? ¿cómo filtrar comentarios no aceptables en foros?"
  },
  {
    "objectID": "01-introduccion.html#ejemplo-reglas-y-aprendizaje",
    "href": "01-introduccion.html#ejemplo-reglas-y-aprendizaje",
    "title": "1  Introducción",
    "section": "1.2 Ejemplo: reglas y aprendizaje",
    "text": "1.2 Ejemplo: reglas y aprendizaje\nLectura de un medidor mediante imágenes. Supongamos que en una infraestructura donde hay medidores análogos (de agua, electricidad, gas, etc.) que no se comunican. ¿Podríamos pensar en utilizar fotos tomadas automáticamente para medir el consumo?\nPor ejemplo, consideramos el siguiente problema (tomado de aquí, ver código y datos):\n\nlibrary(imager)\nlibrary(tidyverse)\nlibrary(gt)\nset.seed(43)\npath_img <- \"../datos/medidor/\"\npath_full_imgs <- list.files(path = path_img, full.names = TRUE)\nmedidor <- load.image(sample(path_full_imgs, 1))\npar(mar = c(1, 1, 1, 1))\nplot(medidor, axes = FALSE)\n\n\n\n\nNótese que las imágenes y videos son matrices o arreglos de valores de pixeles, por ejemplo estas son las dimensiones para una imagen:\n\ndim(medidor)\n\n[1] 193 193   1   3\n\n\nEn este caso, la imagen es de 193 x 193 pixeles y tiene tres canales, o tres matrices de 193 x 193 donde la entrada de cada matriz es la intensidad del canal correspondiente. Buscámos hacer cálculos con estas matrices para extraer la información que queremos. En este caso, construiremos estos cálculos a mano.\nPrimero filtramos (extraemos canal rojo y azul, restamos, difuminamos y aplicamos un umbral):\n\nmedidor_rojo <- medidor |>  R() \nmedidor_azul <- medidor |> B()\nmedidor_1 <- (medidor_rojo - medidor_azul) |> isoblur(5)\naguja <-  medidor_1 |>  imager::threshold(\"90%\", approx = FALSE)\n\n\n\n\n\n\nLogramos extraer la aguja, aunque hay algo de ruido adicional. Una estrategia es extraer la componente conexa más grande (que debería corresponder a la aguja), y luego calcular su orientación. Una manera fácil es encontrar una recta que vaya del centro de la imagen hasta el punto más alejado del centro (aunque quizá puedes pensar maneras más robustas de hacer esto):\n\ncalcular_punta <- function(pixset){\n  centro <- floor(dim(pixset)[1:2] / 2)\n  # segmentar en componentes conexas\n  componentes <- split_connected(pixset)\n  # calcular la más grande\n  num_pixeles <- map_dbl(componentes, sum)\n  ind_maxima <- which.max(num_pixeles)\n  pixset_tbl <- as_tibble(componentes[[ind_maxima]]) |> \n    mutate(dist = (x - centro[1])^2 + (y - centro[2])^2) |> \n    top_n(1, dist)  |> \n    mutate(x_1 = x - centro[1], y_1 = y - centro[2])\n  pixset_tbl[1, ] \n}\n\n\n\n\n\n\nY ahora podemos aplicar el proceso de arriba a todas la imágenes:\n\npath_imgs <- list.files(path = path_img)\n\npath_full_imgs <- list.files(path = path_img, full.names = TRUE)\n# en este caso los datos están etiquetados\ny_imagenes <- path_imgs |> str_sub(1, 3) |> as.numeric()\n# procesar algunas imagenes\nset.seed(82)\nindice_imgs <- sample(1:length(path_full_imgs), 500)\nangulos <- path_full_imgs[indice_imgs] |> \n    map( ~ load.image(.x)) |>  \n    map(~ R(.x) - B(.x)) |> \n    map( ~ isoblur(.x, 5)) |> \n    map( ~ imager::threshold(.x, \"90%\")) |> \n    map( ~ calcular_punta(.x)) |> \n  bind_rows()\n\n\nangulos_tbl <- angulos |> \n  mutate(y_medidor = y_imagenes[indice_imgs])\nggplot(angulos_tbl, \n    aes(x = 180 * atan2(y_1, x_1) / pi + 90, y = y_medidor)) +\n  geom_point() + xlab(\"Ángulo\")\n\n\n\n\nEl desempeño no es muy malo pero tiene algunas fallas grandes. Quizá refinando nuestro pipeline de procesamiento podemos mejorarlo.\n\nPor el contrario, en el enfoque de aprendizaje, comenzamos con un conjunto de datos etiquetado (por una persona, por un método costoso, etc.), y utilizamos alguna estructura general para aprender a producir la respuesta a partir de las imágenes. Por ejemplo, en este caso podríamos una red convolucional sobre los valores de los pixeles de la imagen:\n\nlibrary(keras)\n# usamos los tres canales de la imagen\nimagenes <- map(path_full_imgs, ~ image_load(.x, target_size = c(64, 64)))\n\nLoaded Tensorflow version 2.6.0\n\nimgs_array <-  imagenes |>  map(~ image_to_array(.x)) \nimgs_array <- map(imgs_array, ~ array_reshape(.x, c(1, 64, 64, 3)))\nx <- abind::abind(imgs_array, along = 1)\nset.seed(2311)\nindices_entrena <- sample(1:dim(x)[1], size = 4200)\n# generar lotes de datos de las imágenes originales\ngenerador_1 <- image_data_generator(\n  rescale = 1/255,\n  rotation_range = 5,\n  zoom_range = 0.05,\n  horizontal_flip = FALSE,\n  vertical_flip = FALSE,\n  fill_mode = \"nearest\"\n)\ngenerador_entrena <- flow_images_from_data(\n  x = x[indices_entrena,,,],\n  y = y_imagenes[indices_entrena] / 10,\n  generator = generador_1,\n  shuffle = TRUE,\n  batch_size = 32\n)\n\n\nmodelo_aguja <- keras_model_sequential() |>\n  layer_conv_2d(input_shape = c(64, 64, 3), \n    filters = 32, kernel_size = c(5, 5)) |> \n  layer_max_pooling_2d(pool_size = c(2, 2)) |>\n  layer_conv_2d(filters = 32, kernel_size = c(5, 5)) |> \n  layer_max_pooling_2d(pool_size = c(2, 2)) |> \n  layer_conv_2d(filters = 16, kernel_size = c(3, 3)) |> \n  layer_max_pooling_2d(pool_size = c(2, 2)) |> \n  layer_flatten() |> \n  layer_dropout(0.2) |> \n  layer_dense(units = 100, activation = \"sigmoid\") |>\n  layer_dropout(0.2) |> \n  layer_dense(units = 1, activation = 'linear')\n\nAjustamos el modelo:\n\nmodelo_aguja |> compile(\n  loss = \"mse\",\n  optimizer = optimizer_adam(lr = 0.001),\n  metrics = c('mae')\n)                                                                                                        \n# Entrenar\nmodelo_aguja |> fit(\n  generador_entrena,\n  epochs = 80,\n  verbose = TRUE, \n  validation_data = list(x = x[-indices_entrena,,,], \n                         y = y_imagenes[-c(indices_entrena)] / 10)\n)\nsave_model_hdf5(modelo_aguja, \"cache/modelo-aguja.h5\")\n\n\nmodelo <- load_model_hdf5(\"cache/modelo-aguja.h5\")\nmodelo\n\nModel: \"sequential\"\n________________________________________________________________________________\nLayer (type)                        Output Shape                    Param #     \n================================================================================\nconv2d_2 (Conv2D)                   (None, 60, 60, 32)              2432        \n________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)      (None, 30, 30, 32)              0           \n________________________________________________________________________________\nconv2d_1 (Conv2D)                   (None, 26, 26, 32)              25632       \n________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)      (None, 13, 13, 32)              0           \n________________________________________________________________________________\nconv2d (Conv2D)                     (None, 11, 11, 16)              4624        \n________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)        (None, 5, 5, 16)                0           \n________________________________________________________________________________\nflatten (Flatten)                   (None, 400)                     0           \n________________________________________________________________________________\ndropout_1 (Dropout)                 (None, 400)                     0           \n________________________________________________________________________________\ndense_1 (Dense)                     (None, 100)                     40100       \n________________________________________________________________________________\ndropout (Dropout)                   (None, 100)                     0           \n________________________________________________________________________________\ndense (Dense)                       (None, 1)                       101         \n================================================================================\nTotal params: 72,889\nTrainable params: 72,889\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nY observamos que obtenemos predicciones prometedoras:\n\npreds <- predict(modelo, x[-indices_entrena,,,])\npreds_tbl <- tibble(y = y_imagenes[-c(indices_entrena)] / 10, preds = preds)\nggplot(preds_tbl, aes(x = preds, y = y)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(colour = 'red')\n\n\n\n\nDe forma que podemos resolver este problema con algoritmos generales, sin tener que aplicar métodos sofisticados de procesamiento de imágenes. El enfoque de aprendizaje es particularmente efectivo cuando hay cantidades grandes de datos poco ruidosos, y aunque en este ejemplo los dos enfoques dan resultados razonables, en procesamiento de imágenes es cada vez más común usar redes neuronales grandes para resolver este tipo de problemas."
  },
  {
    "objectID": "01-introduccion.html#medicioncostosa",
    "href": "01-introduccion.html#medicioncostosa",
    "title": "1  Introducción",
    "section": "1.3 Ejemplo: mediciones costosas",
    "text": "1.3 Ejemplo: mediciones costosas\nEn algunos casos, el estándar de la medición que nos interesa es uno que es costoso de cumplir: a veces se dice que etiquetar los datos es costoso. Un ejemplo es producir las estimaciones de ingreso trimestral de un hogar que se recolecta en la ENIGH (ver aquí). En este caso particular, se utiliza esta encuesta como datos etiquetados para poder estimar el ingreso de otros hogares que no están en la muestra del ENIGH, pero para los que se conocen características de las vivienda, características de los integrantes, y otras medidas que son más fácilmente recolectadas en encuestas de opinión.\nVeremos otro ejemplo: estimar el valor de mercado de las casas en venta de una región. Es posible que tengamos un inventario de casas con varias de sus características registradas, pero producir estimaciones correctas de su valor de mercado puede requerir de inspecciones costosas de expertos, o tomar aproximaciones imprecisas de esta cantidad (por ejemplo, cuál es el precio ofertado).\nUtilizaremos datos de casas que se vendieron en Ames, Iowa en cierto periodo. En este caso, conocemos el valor a la que se vendió una casa. Buscamos producir una estimación para otras casas para las cuales conocemos características como su localización, superficie en metros cuadrados, año de construcción, espacio de estacionamiento, y así sucesivamente. Estas medidas son más fáciles de recolectar, y quisiéramos producir una estimación de su precio de venta en términos de estas medidas.\nEn este ejemplo intentaremos una forma simple de predecir.\n\nlibrary(tidymodels)\nlibrary(patchwork)\nsource(\"../R/casas_traducir_geo.R\")\nset.seed(68821)\n# dividir muestra\ncasas_split <- initial_split(casas, prop = 0.75)\n# obtener muestra de entrenamiento\ncasas_entrena <- training(casas_split)\n# graficar\ng_1 <- ggplot(casas_entrena, aes(x = precio_miles)) +\n  geom_histogram()\ng_2 <- ggplot(casas_entrena, aes(x = area_hab_m2, \n                          y = precio_miles, \n                          colour = condicion_venta)) +\n  geom_point() \ng_1 + g_2\n\n\n\n\nLa variable de condición de venta no podemos utilizarla para predecir, pues sólo la conocemos una vez que la venta se hace. Podemos ver en lugar de eso solamente las de condición normal. Consideramos además del área habitable, por ejemplo, la calidad general de terminados:\n\nggplot(casas_entrena |>  \n       filter(condicion_venta == \"Normal\") |>  \n       mutate(calidad_grupo = \n        cut(calidad_gral, breaks = c(0, 5, 7, 8, 10))), \n  aes(x = area_hab_m2, \n      y = precio_miles,\n      colour = calidad_grupo)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, formula = \"y ~ x\")\n\n\n\n\nPrecio vs área y calidad\n\n\n\ng_1\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nPrecio vs área y calidad\n\n\n\n\nVemos que estas dos variables que hemos usado explican buena parte de la variación de los precios de las casas. Podemos examinar otras variables como la existencia y tamaño del garage:\n\nggplot(casas_entrena |>  filter(condicion_venta == \"Normal\"),\n       aes(x = area_hab_m2, y = precio_miles, colour = area_garage_m2)) +\n  geom_point(alpha = 0.5) + facet_wrap(~ (area_garage_m2 == 0))\n\n\n\n\nY quizá podríamos proponer una fórmula simple de la forma:\n\\[Precio = a_{calidad} + b_{calidad}\\textrm{Area} + c \\textrm{AreaGarage} + d\\textrm{TieneGarage}\\]\ndonde los valores de \\(a_{calidad}, b_{calidad}, c, d\\) podríamos estimarlos de los datos. La pendiente de Area dependende de la calificación de la calidad de los terminados.\nNuestro proceso comenzaría entonces construir los datos para usar en el modelo:\n\nreceta_casas <- \n  recipe(precio_miles ~ area_hab_m2 + calidad_gral + \n           area_garage_m2, \n         data = casas_entrena) |>  \n  step_cut(calidad_gral, breaks = c(3, 5, 6, 7, 8)) |>  \n  step_normalize(starts_with(\"area\")) |> \n  step_mutate(tiene_garage = ifelse(area_garage_m2 > 0, 1, 0)) |> \n  step_dummy(calidad_gral) |> \n  step_interact(terms = ~ area_hab_m2:starts_with(\"calidad_gral\")) \n\nDefinimos el tipo de modelo que queremos ajustar, creamos un flujo y ajustamos\n\n# modelo\ncasas_modelo <- linear_reg() |> \n  set_engine(\"lm\")\n# flujo\nflujo_casas <- workflow() |> \n  add_recipe(receta_casas) |> \n  add_model(casas_modelo)\n# ajustar flujo\najuste <- fit(flujo_casas, casas_entrena)\najuste\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_cut()\n• step_normalize()\n• step_mutate()\n• step_dummy()\n• step_interact()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n                       (Intercept)                         area_hab_m2  \n                           111.124                              22.505  \n                    area_garage_m2                        tiene_garage  \n                            12.014                               3.230  \n               calidad_gral_X.3.5.                 calidad_gral_X.5.6.  \n                            30.104                              54.623  \n               calidad_gral_X.6.7.                 calidad_gral_X.7.8.  \n                            79.565                             119.639  \n              calidad_gral_X.8.10.   area_hab_m2_x_calidad_gral_X.3.5.  \n                           217.099                              -7.942  \n area_hab_m2_x_calidad_gral_X.5.6.   area_hab_m2_x_calidad_gral_X.6.7.  \n                             2.839                              14.141  \n area_hab_m2_x_calidad_gral_X.7.8.  area_hab_m2_x_calidad_gral_X.8.10.  \n                            14.221                              -1.421  \n\n\nY ahora podemos hacer predicciones:\n\nset.seed(8)\ncasas_prueba <- testing(casas_split) \nejemplos <- casas_prueba|> sample_n(5)\npredict(ajuste, ejemplos) |> \n  bind_cols(ejemplos |> select(precio_miles, area_hab_m2)) |> \n  arrange(desc(precio_miles)) |> gt() |> \n  fmt_number(columns = everything(), decimals = 1)\n\n\n\n\n\n  \n  \n    \n      .pred\n      precio_miles\n      area_hab_m2\n    \n  \n  \n    242.3\n275.0\n152.9\n    177.3\n181.0\n155.6\n    169.3\n175.5\n132.1\n    123.1\n133.0\n117.8\n    115.6\n128.5\n90.2\n  \n  \n  \n\n\n\n\nY finalmente podemos evaluar nuestro modelo. En este casos mostramos diversas métricas como ejemplo:\n\nmetricas <- metric_set(mape, mae, rmse)\nmetricas(casas_prueba |> bind_cols(predict(ajuste, casas_prueba)), \n     truth = precio_miles, estimate = .pred) |> gt() |> \n  fmt_number(columns = where(is_double), decimals = 1)\n\n\n\n\n\n  \n  \n    \n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    mape\nstandard\n14.1\n    mae\nstandard\n23.4\n    rmse\nstandard\n33.3\n  \n  \n  \n\n\n\n\n\ncasas_prueba_f <- filter(casas_prueba,\n  condicion_venta %in% c(\"Normal\", \"Partial\", \"Abnorml\"))\nggplot(casas_prueba_f |>\n       bind_cols(predict(ajuste, casas_prueba_f)),\n       aes(x = .pred, y = precio_miles)) +\n  geom_point() +\n  geom_abline(colour = \"red\") + facet_wrap(~ condicion_venta)\n\n\n\n\nEste modelo tiene algunos defectos y todavía tiene error considerablemente grande. La mejora sin embargo podemos cuantificarla con un modelo base o benchmark. En este caso utilizamos el siguiente modelo simple, cuya predicción es el promedio de entrenamiento:\n\n# nearest neighbors es grande, así que la predicción\n# es el promedio de precio en entrenamiento\ncasas_promedio <- nearest_neighbor(\n    neighbors = 1000, weight_func = \"rectangular\") |>\n  set_mode(\"regression\") |> \n  set_engine(\"kknn\")\nworkflow_base <- workflow() |> \n  add_recipe(receta_casas) |> \n  add_model(casas_promedio)\najuste_base <- fit(workflow_base, casas_entrena)\nmetricas(casas_prueba |> bind_cols(predict(ajuste_base, casas_prueba)), \n     truth = precio_miles, estimate = .pred)|> gt() |> \n  fmt_number(columns = where(is_double), decimals = 1)\n\n\n\n\n\n  \n  \n    \n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    mape\nstandard\n33.4\n    mae\nstandard\n54.8\n    rmse\nstandard\n77.2"
  },
  {
    "objectID": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "href": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "title": "1  Introducción",
    "section": "1.4 Aprendizaje supervisado y no supervisado",
    "text": "1.4 Aprendizaje supervisado y no supervisado\nLas tareas de aprendizaje se dividen en dos grandes partes: aprendizaje supervisado y aprendizaje no supervisado.\nEn Aprendizaje supervisado buscamos construir un modelo o algoritmo para predecir o estimar un target o una respuesta a partir de ciertas variables de entrada.\nPredecir y estimar, en este contexto, se refieren a cosas similares. Generalmente se usa predecir cuando se trata de variables que no son observables ahora, sino en el futuro, y estimar cuando nos interesan variables actuales que no podemos observar ahora por costos o por la naturaleza del fenómeno.\nPor ejemplo, para identificar a los clientes con alto riesgo de impago de tarjeta de crédito, utilizamos datos históricos de clientes que han pagado y no han pagado. Con estos datos entrenamos un algoritmo para detectar anticipadamente los clientes con alto riesgo de impago.\nUsualmente dividimos los problemas de aprendizaje supervisado en dos tipos, dependiendo de la variables salida:\n\nProblemas de regresión: cuando la salida es una variable numérica. El ejemplo de estimación de ingreso es un problema de regresión\nProblemas de clasificación: cuando la salida es una variable categórica. El ejemplo de detección de dígitos escritos a manos es un problema de clasificación.\n\nEn contraste, en Aprendizaje no supervisado no hay target o variable respuesta. Buscamos modelar y entender las relaciones entre variables y entre observaciones, o patrones importantes o interesantes en los datos.\nLos problemas supervisados tienen un objetivo claro: hacer las mejores predicciones posibles bajo ciertas restricciones. Los problemas no supervisados tienden a tener objetivos más vagos, y por lo mismo pueden ser más difíciles."
  },
  {
    "objectID": "02-principios-supervisado.html",
    "href": "02-principios-supervisado.html",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "",
    "text": "En esta sección examinaremos algunos principios teóricos y de metodología para el aprendizaje supervisado."
  },
  {
    "objectID": "02-principios-supervisado.html#población-y-pérdida",
    "href": "02-principios-supervisado.html#población-y-pérdida",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.1 Población y pérdida",
    "text": "2.1 Población y pérdida\nSupongamos que tenemos una población grande de observaciones potenciales de la forma\n\\[(x_1, x_2, \\ldots, x_p, y) \\]\nY para esa población nos interesa predecir una variable respuesta \\(y\\) numérica en términos de variables de entrada disponibles \\(x = (x_1,x_2,\\ldots, x_p)\\):\n\\[(x_1, x_2, \\ldots, x_p) \\to y\\]\nEl proceso que produce la salida \\(y\\) a partir de las entradas es típicamente muy complejo y dificíl de describir de forma mecanística (por ejemplo, el ingreso dadas características de los hogares).\n\nEjemplo\nPara ilustrar esta discusión teórica, consideraremos datos simulados. La población está dada por el siguiente proceso generador de datos:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(gt)\ngenera_datos <- function(n = 500, tipo = NULL){\n  dat_tbl <- tibble(nse = runif(n, 0, 100)) |>\n    mutate(estudio_años = floor(rnorm(n, 1.5 * sqrt(nse), 1))) |>\n    mutate(estudio_años = pmax(0, pmin(17, estudio_años))) |> \n    mutate(habilidad = rnorm(n, 100 + 0.1 * nse, 1)) |> \n    mutate(z = 100 + (habilidad/100) * ( 20 * nse + 5 * (estudio_años))) |> \n    mutate(ingreso = pmax(0, 0.2*(z + rnorm(n, 0, 150))))\n  obs_tbl <- dat_tbl |> \n    mutate(tipo = tipo, id = 1:n)\n  obs_tbl |> select(id, tipo, x = estudio_años, y = ingreso)\n}\n\nTenemos una sola entrada y una respuesta numérica, y una muestra se ve como sigue:\n\nset.seed(1234)\ndatos_tbl <- genera_datos(n = 500, tipo = \"entrena\")\nggplot(datos_tbl, aes(x = x, y = y)) + geom_jitter(width = 0.3) +\n  xlab(\"Años de estudio\") + ylab(\"Ingreso anual (miles)\")\n\n\n\n\n\nBuscamos construir una función \\(f\\) tal que si observamos cualquier \\(x = (x_1, x_2, \\ldots, x_p) \\to y\\), entonces nuestra predicción es\n\\[\\hat{y} = f(x_1, x_2, \\ldots, x_p) = f(x).\\]\nCon esta regla o algoritmo \\(f\\) queremos predecir con buena precisión el valor de \\(y\\). Esta \\(f\\), como explicamos antes, puede ser producida de muy distintas maneras (experiencia, reglas a mano, datos, etc.)\nNuestra primera tarea es definir qué quiere decir predecir con buena precisión.\nPara hacer esto tenemos que introducir una medida del error, que llamamos en general función de pérdida.\n\n\n\n\n\n\nFunción de pérdida y error de predicción\n\n\n\nSi el verdadero valor observado es \\(y\\) y nuestra predicción es \\(f(x)\\), denotamos la pérdida asociada a esta observación como\n\\[L(y, f(x))\\]\nPara medir el desempeño general de la regla \\(f\\), consideramos su valor esperado, el error de predicción, que es el promedio sobre toda la población:\n\\[Err(f) = E[L(y, f(x))]\\]\nEste es el error que obtendríamos promediando las pérdidas sobre toda la población de interés.\n\n\nObservación: Para fijar ideas, podríamos usar por ejemplo la pérdida absoluta \\(L(y, f(x)) = |y - f(x)|\\) o la pérdida cuadrática \\(L(y, f(x)) = (y - f(x))^2\\).\nAl menos en teoría, podemos encontrar una \\(f\\) que minimiza esta pérdida:\n\n\n\n\n\n\nPredictor óptimo\n\n\n\nPara una población dada, el predictor óptimo (teórico) es\n\\[f^* = \\underset{f}{\\mathrm{argmin}} E[L(y, f(x))].\\]\nEs decir: el mínimo error posible que podemos obtener es \\(Err(f^*)\\). Para cualquier otro predictor \\(f\\) tenemos que \\(Err(f) \\geq Err(f^*).\\)\n\n\nPor ejemplo si usamos la pérdida cuadrática \\(L(y, f(x)) = (y - f(x))^2\\), entonces puede mostrarse que\n\\[f^*(x) = E(y | x)\\]\nde forma que \\(f^*\\) es la media condicional de la \\(y\\) dado que sabemos que las entradas son \\(x\\). Si usáramos la pérdida absoluta \\(L(y, f(x)) = |y - f(x)|\\) entonces\n\\[f^*(x) = \\textrm{mediana}(y|x).\\]\nDistintas funciones de pérdida dan distintas soluciones teóricas. Por ejemplo, si existen valores atípicos en \\(y\\) producidos por errores de registro o medición, usar la pérdida absoluta puede dar mejores resultados que la cuadrática, que tiende a dar mayor peso a errores grandes.\nObservaciones:\n\nPodemos ver nuestra tarea entonces como una de ajuste de curvas: queremos aproximar tan bien como sea posible la función \\(f^*(x)\\).\nNo es simple decidir qué función de pérdida debería utilizarse para un problema dado de predicción.\nGeneralmente es una combinación de costos/beneficios del problema que tratamos, conveniencia computacional, y cómo se comportan los errores de nuestros predictores bajo distintas pérdidas.\nMuchas veces es mejor considerar el problema de selección de la pérdida desde dos ángulos: el primero es computacional y de propiedades de la predicción, y el segundo tiene que ver con costos y beneficios asociados al problema que queremos resolver. Para el primero, alguna de las pérdidas estándar (como las que vimos arriba, cuadrática y absoluta, ologarítmica) son usualmente suficiente. En el segundo enfoque, el análisis es generalmente involucra más aspectos particulares del problema y generalmente tiene que hacerse de manera ad-hoc.\n\n\n\nEjemplo\nSupongamos que nos interesa minimizar la pérdida cuadrática. Si tomamos una muestra muy grande (para este problema), podemos aproximar la predicción óptima directamente. Abajo graficamos nuestra muestra chica de datos junto con una buna aproximación del predictor óptimo:\n\npoblacion_tbl <- genera_datos(n = 50000, tipo = \"poblacion\")\n# calcular óptimo\npreds_graf_tbl <- poblacion_tbl |> \n  group_by(x) |> # condicionar a x\n  summarise(.pred = mean(y)) |> # mediana, pues usamos pérdida absoluta\n  mutate(predictor = \"_óptimo\")\n# graficar con una muestra grande\nggplot(datos_tbl, aes(x = x)) +\n  geom_jitter(aes(y = y), colour = \"red\") + \n  geom_line(data = preds_graf_tbl, aes(y = .pred, colour = predictor), size = 1.1) +\n  xlab(\"Años de estudio\") + ylab(\"Ingreso anual (miles)\")"
  },
  {
    "objectID": "02-principios-supervisado.html#estimando-el-desempeño-y-datos-de-prueba",
    "href": "02-principios-supervisado.html#estimando-el-desempeño-y-datos-de-prueba",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.2 Estimando el desempeño y datos de prueba",
    "text": "2.2 Estimando el desempeño y datos de prueba\nPara obtener una estimación de la pérdida para una función \\(f\\) que usamos para hacer predicciones, podemos tomar una muestra de datos del proceso generador:\n\\[{\\mathcal T} = \\{(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}), (\\mathbf{x}^{(2)}, \\mathbf{y}^{(2)}), \\ldots, (\\mathbf{x}^{(m)}, \\mathbf{y}^{(m)})\\},\\]\nCompararíamos entonces las respuestas observadas \\(\\mathbf{y^{(i)}}\\) con las predicciones \\(f(\\mathbf{x^{(i)}})\\). Ahora resumimos evaluando el error promedio sobre los datos de prueba. El error de prueba de \\(f\\) es\n\\[ \\widehat{Err}(f) = \\frac{1}{m} \\sum_{i=1}^m L(\\mathbf{y}^{(i)} , f(\\mathbf{x}^{(i)}))\\]\nPor ejemplo, si usamos la pérdida cuadrática,\n\\[ \\widehat{Err}(f) = \\frac{1}{m} \\sum_{i=1}^m (\\mathbf{y}^{(i)} - f(\\mathbf{x}^{(i)}))^2\\]\nSi \\(m\\) es grande, entonces tenemos por la ley de los grandes números que\n\\[Err(f) \\approx \\widehat{Err} (f)\\]\nPodemos también estimar el error de estimación de \\(\\widehat{Err}(f)\\) con técnicas estándar, por ejemplo bootstrap o aproximación normal.\nObervación: nótese que en estos cálculos no es necesario hacer ningún supuesto acerca de \\(f\\), que en este argumento está fija y no utiliza la muestra de prueba.\n\nEjemplo: óptimo\nSupongamos que \\(f\\) es el predictor óptimo que obtuvimos arriba (pero esto aplica para cualquier otra función \\(f\\) que usemos para hacer predicciones). Tomamos una muestra de prueba, y evaluamos usando la raíz de la pérdida cuadrática media:\n\nprueba_tbl <- genera_datos(n = 2000, tipo = \"prueba\")\neval_tbl <- prueba_tbl |>  \n  left_join(preds_graf_tbl, by = \"x\") \nresumen_tbl <- eval_tbl |>  \n  group_by(predictor, tipo) |> \n  rmse(truth = y, estimate = .pred) \nfmt_resumen <- function(resumen_tbl){\n  resumen_tbl |> \n    select(-.estimator) |> \n    pivot_wider(names_from = tipo, values_from = .estimate) |> \n    gt() |> \n    fmt_number(where(is_double), decimals = 0)\n}\nfmt_resumen(resumen_tbl)\n\n\n\n\n\n  \n  \n    \n      predictor\n      .metric\n      prueba\n    \n  \n  \n    _óptimo\nrmse\n49\n  \n  \n  \n\n\n\n\nEste es nuestro error de prueba. Como la muestra de prueba no es muy grande, podríamos usar un método estándar para estimar su precisión, por ejemplo con bootstrap.\n\n\nEjemplo: regla\nAhora probemos con otro predictor, por ejemplo, supongamos que estamos usando la regla de “cada año de escolaridad aumenta ingresos potenciales en 20 unidades”, un predictor construido con reglas manuales que es\n\nf_regla <- function(x){\n  20 * x\n}\n\nAbajo lo graficamos en comparación con el modelo óptimo:\n\naños_x <- tibble(x = seq(0, 17, by = 0.5))\npreds_regla_tbl <- años_x |> \n  mutate(.pred = f_regla(x), predictor = \"regla\")\npreds_graf_tbl <- bind_rows(preds_regla_tbl, preds_graf_tbl)\nggplot(datos_tbl, aes(x = x)) +\n  geom_point(aes(y = y), colour = \"red\", alpha = 0.2) +\n  geom_line(data = preds_graf_tbl, aes(y = .pred, colour = predictor), size = 1.1) \n\n\n\n\n\neval_tbl <- prueba_tbl |>  \n  left_join(preds_graf_tbl, by = \"x\") \nresumen_tbl <- eval_tbl |>  \n  group_by(predictor, tipo) |> \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(resumen_tbl)\n\n\n\n\n\n  \n  \n    \n      predictor\n      .metric\n      prueba\n    \n  \n  \n    _óptimo\nrmse\n49\n    regla\nrmse\n91\n  \n  \n  \n\n\n\n\nObserva que el error es considerablemente mayor que el error que obtuvimos con el predictor óptimo del ejemplo anterior. Quisiéramos buscar algoritmos que tengan mejor desempeño aprendiendo de datos anteriores."
  },
  {
    "objectID": "02-principios-supervisado.html#aprendizaje",
    "href": "02-principios-supervisado.html#aprendizaje",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.3 Aprendizaje supervisado",
    "text": "2.3 Aprendizaje supervisado\nEn aprendizaje supervisado, buscamos construir la función \\(f\\) de manera automática usando datos. Supongamos entonces que tenemos un conjunto de datos etiquetados (sabemos la \\(y\\) correspondiente a cada \\(x\\)):\n\\[{\\mathcal L}=\\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}\\]\nque llamamos conjunto de entrenamiento.\nUn algoritmo de aprendizaje (aprender de los datos automáticamente) es una regla que asigna a cada conjunto de entrenamiento \\({\\mathcal L}\\) una función \\(\\hat{f}\\):\n\\[{\\mathcal L} \\to \\hat{f} = f_{\\mathcal L} \\]\nUna vez que construimos la función \\(\\hat{f}\\), podemos hacer predicciones.\nEl desempeño del predictor particular \\(\\hat{f}\\) se mide igual que antes: observamos otra muestra \\({\\mathcal T}\\), que llamamos muestra de prueba,\n\\[{\\mathcal T} = \\{(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}), (\\mathbf{x}^{(2)}, \\mathbf{y}^{(2)}), \\ldots, (\\mathbf{x}^{(m)}, \\mathbf{y}^{(m)})\\},\\]\ny calculamos el error de prueba. Si suponemos que \\(m\\) es suficientemente grande:\n\\[ \\widehat{Err}(\\hat{f}) = \\frac{1}{m} \\sum_{i=1}^m L(\\mathbf{y}^{(i)} , \\hat{f}(\\mathbf{x}^{(i)})) \\]\nes una buena aproximación del error de predicción \\(Err(\\hat{f})\\).\nAdicionalmente, definimos otra cantidad de menor interés, el error de entrenamiento, como\n\\[\\overline{err} = \\frac{1}{N}\\sum_{i=1}^N L(y^{(i)} , \\hat{f}(x^{(i)})).\\]\nque es una medida de qué tan bien se ajusta a \\(\\hat{f}\\) a los datos con los que se entrenó \\(\\hat{f}\\). Usualmente esta cantidad no es apropiada para medir el desempeño de un predictor, pues el algoritmo \\(\\hat{f}\\) incluye las “respuestas” \\(y_i\\) en su construcción, de forma que tiende a ser una estimación optimista del error de predicción.\n\nEjemplo: vecinos más cercanos\nConsideremos usar un método de \\(k\\)-vecinos más cercanos para resolver este problema. Este método es simple: si queremos hacer una predicción en las entradas \\(x\\), buscamos los puntos de entrenamiento con entradas \\(x^{(i)}\\) más cercanas a \\(x\\), que denotamos como \\(N_k(x)\\). Tomamos las \\(y\\) correspondientes a estas \\(x\\) y las usamos para hacer nuestra predicción:\n\\[f_2(x) = \\frac{1}{k}\\sum_{x^{(i)} \\in N_k(x)} y^{(i)}\\]\nPrimero obtendremos una muestra de entrenamiento:\n\nset.seed(12)\nentrena_tbl <- genera_datos(n = 20, tipo = \"entrena\")\n\nEn nuestro ejemplo, en lugar de usar un número fijo de vecinos, utilizaremos 10% de los datos más cercanos al punto donde queremos predecir:\n\n# modelo\nmodelo_kvecinos <- nearest_neighbor(\n    neighbors = nrow(entrena_tbl) * 0.1, \n    weight_func = \"gaussian\") |> \n  set_mode(\"regression\") |> \n  set_engine(\"kknn\")\n# preprocesamiento\nreceta <- recipe(y ~ x, data = entrena_tbl |> select(x, y))\n# flujo\nflujo <- workflow() |> \n  add_recipe(receta) |> \n  add_model(modelo_kvecinos)\n# Ajustamos flujo\nflujo_ajustado_vecinos <- fit(flujo, entrena_tbl)\n\nHacemos predicciones y calculamos el error:\n\neval_tbl <- bind_rows(prueba_tbl, entrena_tbl) \nresumen_vmc_tbl <- \n  predict(flujo_ajustado_vecinos, eval_tbl) |> \n  mutate(predictor = \"vecinos\") |> \n  bind_cols(eval_tbl) |> \n  group_by(predictor, tipo) |> \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(resumen_vmc_tbl)\n\n\n\n\n\n  \n  \n    \n      predictor\n      .metric\n      entrena\n      prueba\n    \n  \n  \n    vecinos\nrmse\n36\n65\n  \n  \n  \n\n\n\n\nEl error de prueba, que es el que nos interesa hacer chico, es considerablemente grande. Si graficamos podemos ver el problema:\n\npreds_vmc <- predict(flujo_ajustado_vecinos, años_x) |> \n  bind_cols(años_x) |> mutate(predictor = \"vecinos\")\npreds_graf_tbl <- bind_rows(preds_vmc, preds_graf_tbl |> \n  filter(predictor == \"_óptimo\"))\ng_1 <- ggplot(entrena_tbl, aes(x = x)) +\n  geom_line(data = preds_graf_tbl |> filter(predictor != \"regla\"), \n            aes(y = .pred, colour = predictor), size = 1.1) +\n  geom_point(aes(y = y), colour = \"red\") +\n  labs(subtitle = \"Óptimo vs ajustado\")\ng_1\n\n\n\n\nDonde vemos que este método intenta interpolar los datos, capturando ruido y produciendo variaciones que lo alejan del modelo óptimo. Esto lo notamos en lo siguiente:\n\nHay una brecha grande entre el error de entrenamiento y el error predictivo.\nEsta estimación de vecinos más cercanos es muy dependiente de la muestra de entrenamiento que obtengamos, pues intenta casi interpolar los datos. Esto sugiere alta variabilidad de las predicciones dependiendo de la muestra particular de entrenamiento que utilizamos.\nDecimos que este predictor está sobreajustado.\n\n\n\nEjemplo: regresión lineal\nAhora intentaremos con un modelo lineal. En este caso, utilizamos un predictor de la forma\n\\[f(x) = \\beta_0 + \\beta_1x\\]\nUsamos la muestra de entrenamiento para encontrar la \\(\\beta_0\\) y \\(\\beta_1\\) que minimizar el error sobre los datos disponibles de entrenamiento, lo cual es un problema de optimización relativamente fácil. Usamos entonces\n\\[\\hat{f}(x) =\\hat{\\beta}_0 + \\hat{\\beta}_1 x\\]\npara hacer nuestras predicciones.\n\nmodelo_lineal <- linear_reg() |> \n  set_mode(\"regression\") |> \n  set_engine(\"lm\")\nflujo_lineal <- workflow() |> \n  add_recipe(receta) |> \n  add_model(modelo_lineal)\n# Ajustamos\nflujo_ajustado_lineal <- fit(flujo_lineal, entrena_tbl)\n\nHacemos predicciones y calculamos el error:\n\neval_tbl <- bind_rows(prueba_tbl, entrena_tbl) \nresumen_lineal_tbl <- \n  predict(flujo_ajustado_lineal, eval_tbl) |> \n  mutate(predictor = \"lineal\") |> \n  bind_cols(eval_tbl) |> \n  group_by(predictor, tipo) |> \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(bind_rows(resumen_vmc_tbl, resumen_lineal_tbl))\n\n\n\n\n\n  \n  \n    \n      predictor\n      .metric\n      entrena\n      prueba\n    \n  \n  \n    vecinos\nrmse\n36\n65\n    lineal\nrmse\n49\n56\n  \n  \n  \n\n\n\n\nY el desempeño de este método es mejor que vecinos más cercanos (ver columna de prueba).\n\npreds_1 <- predict(flujo_ajustado_lineal, tibble(x = 0:17)) |> \n  bind_cols(tibble(x = 0:17, predictor = \"lineal\"))\npreds_graf_tbl <- bind_rows(preds_1, preds_graf_tbl)\ng_1 <- ggplot(datos_tbl, aes(x = x)) +\n  geom_point(aes(y = y), colour = \"red\", alpha = 0.1) +\n  geom_line(data = preds_graf_tbl |> filter(predictor %in% c(\"_óptimo\", \"lineal\")), \n            aes(y = .pred, colour = predictor), size = 1.1) +\n  labs(subtitle = \"Óptimo vs ajustado\")\ng_1\n\n\n\n\nEn este caso:\n\nNo hay brecha tan grande entre el error de entrenamiento y el error predictivo\nObservamos patrones claros de desajuste: el predictor lineal no captura el patrón curvo que presentan los datos: en la parte media de las \\(x\\) tiende a producir predicciones demasiado altas y lo contario ocurre en los extremos\nDecimos que esté modelo presenta subajuste."
  },
  {
    "objectID": "02-principios-supervisado.html#entendiendo-el-error-de-predicción",
    "href": "02-principios-supervisado.html#entendiendo-el-error-de-predicción",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.4 Entendiendo el error de predicción",
    "text": "2.4 Entendiendo el error de predicción\nEstos dos ejemplos de predictores tienen mal desempeño (comparado con el óptimo por distintas razones. Para entender qué pasa, consideramos los residuales de cada ajuste, para un caso de prueba:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x})\\]\nEsta cantidad puede tener un valor positivo o negativo grande, lo que indica errores grandes. Sea \\(f^*\\) el predictor óptimo que explicamos arriba. Entonces, en primer lugar:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) = \\underbrace{(f^* (\\mathbf{x}) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}))}_\\text{reducible} + \\underbrace{(\\mathbf{y}- f^*(\\mathbf{x}))}_\\text{irreducible}.\\]\ndonde vemos que si las dos cantidades de la derecha están cercanas a cero, entonces el residual es cercano a cero (la predicción es precisa):\n\nError irreducible: no depende de nuestro algoritmo, sino de la información que tenemos en \\(x\\) para predecir \\(y\\). Si queremos hacer esté error más chico, necesitamos incluir otras variables \\(x\\) relevantes para predecir \\(y\\).\nError reducible: qué tan lejos nuestro método está del óptimo. Podemos mejorar este error seleccionando nuestra muestra de entrenamiento y método de predicción \\(\\hat{f}\\) de manera adecuada.\n\nEn nuestros dos ejemplos anteriores, el error reducible era considerablemente grande (como podemos verificar comparando con el predictor óptimo, que sólo sufre de error irreducible). Pero la razón por la que ese error reducible es grande es diferente en cada caso.\nPara explicar la diferencia, podemos considerar \\(f_{lim},\\) el predictor que obtendríamos con nuestro método si ajustáramos nuestro método con la población completa, de manera que \\(\\hat{f_{\\mathcal{L}}}\\to f_{\\lim}\\) cuando el tamaño de la muestra de entrenamiento \\({\\mathcal{L}}\\) se hace muy grande.\nPodemos refinar nuestra descomposición y escribir:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) = \\underbrace{f^* (\\mathbf{x}) - f_{\\lim}(\\mathbf{x})}_\\text{sesgo-especificacion} +\n  \\underbrace{f_{\\lim}(\\mathbf{x}) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x})}_\\text{error-estimacion} +\n  \\underbrace{y - f^*(\\mathbf{x})}_\\text{irreducible}.\\]\nEl error reducible ahora se descompone en dos partes:\n\nEl sesgo de especificacion: que se debe a la incapacidad de nuestro modelo de capturar la forma del predictor óptimo, incluso conociendo toda la población. Este término no depende de la muestra de entrenamiento: depende de la capacidad de nuestro método para aprender en condiciones ideales.\nEl error de estimación: este error resulta de que tenemos información limitada de la población, y nuestro ajuste se aleja de lo que obtendríamos con información completa. Esta parte del error varía dependiendo de la muestra particular de entrenamiento que utilizamos.\n\nEn nuestros dos ejemplos, intuímos que vecinos más cercanos sufre más de error de estimación y regresión lineal de sesgo de especificación, lo cual verificamos más adelante.\nPodemos refinar aún más nuestra descomposición considerando qué pasa con distintas muestras del mismo tamaño para entender mejor el error de estimación. Si consideramos el valor esperado de nuestra predicción a lo largo de las posibles muestras \\(\\mathcal L\\) que podemos extraer, descomponemos el segundo término como:\n\\[\\hat{f_{\\mathcal{L}}}(\\mathbf{x}) - f_{\\lim}(\\mathbf{x})  = \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) -  E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})) + E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})) - f_{\\lim}(\\mathbf{x}) \\]\ndonde el valor esperado es sobre todas las muestras de entrenamiento de un tamaño fijo \\(n\\) que podríamos obtener. El primer término puede llamarse variabilidad, mientras que el segundo es el sesgo que obtenemos al usar una muestra \\(n\\) finita (para algunos métodos, el segundo término puede ser igual a cero). Desde este punto de vista, podemos hacer también la descomposición:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) = \\underbrace{f^* (\\mathbf{x}) - f_{lim}(\\mathbf{x})}_\\text{sesgo-especificacion}  + \\underbrace{f_{lim}(\\mathbf{x}) - E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x}))}_\\text{sesgo-estimacion} +   \\underbrace{E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x})}_\\text{variabilidad} + \\underbrace{ \\mathbf{y} - f^*(\\mathbf{x})}_\\text{irreducible}.\\]\nTenemos entonces cuatro términos:\n\nEl sesgo de especificación mide la capacidad del modelo de utilizar datos de muestras cada vez más grandes. No depende de una muestra particular ni su tamaño.\nEl sesgo de estimación mide en promedio qué tan lejos está la estimación del ideal con datos completos, y depende de la naturaleza de la muestra de entrenamiento, incluyendo su tamaño.\nLa variabilidad es el único término que depende de la muestra particular que usamos. También depende del tamaño de muestra que utilizamos.\n\nLos dos primeros términos usualmente se agrupan en un sólo término de sesgo, y obtenemos la siguiente definición usual:\n\n\n\n\n\n\nDescomposición sesgo-varianza\n\n\n\nEl error total (la diferencia entre observado y nuestra predicción) se descompone como:\n\\[\\mathbf{y} - \\hat{f_{\\mathcal{L}}}(\\mathbf{x}) = \\underbrace{f^* (\\mathbf{x}) - E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x}))}_\\text{sesgo} +   \\underbrace{E(\\hat{f_{\\mathcal{L}}}(\\mathbf{x})) - \\hat{f_{\\mathcal{L}}}(\\mathbf{x})}_\\text{variabilidad} + \\underbrace{y - f^*(\\mathbf{x})}_\\text{irreducible}.\\]\n\n\nQue explica qué sucede con distintas posibles muestras de entrenamiento de tamaño fijo. El sesgo en este caso significa en promedio qué tan lejos nuestro predictor está del óptimo, y la variabilidad qué tanto puede variar nuestra predicción con respecto al promedio."
  },
  {
    "objectID": "02-principios-supervisado.html#ejemplo-fuentes-de-error",
    "href": "02-principios-supervisado.html#ejemplo-fuentes-de-error",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.5 Ejemplo: fuentes de error",
    "text": "2.5 Ejemplo: fuentes de error\nVamos a ver qué sucede con nuestros dos métodos si utilizamos una muestra grande:\n\nmuestra_grande_tbl <- sample_n(poblacion_tbl, 10000) |> \n  mutate(tipo = \"entrena\")\nmodelo_kvecinos <- nearest_neighbor(\n    neighbors = nrow(muestra_grande_tbl) * 0.10, \n    weight_func = \"gaussian\") |> \n  set_mode(\"regression\") |> \n  set_engine(\"kknn\")\n# Ajustamos (no es necesario usar la población completa para este ejemplo)\nflujo_vecinos <- workflow() |> \n  add_recipe(receta) |> \n  add_model(modelo_kvecinos)\nflujo_ajustado_vecinos_limite <- fit(flujo_vecinos, muestra_grande_tbl)\nflujo_ajustado_lineal_limite <- fit(flujo_lineal, muestra_grande_tbl)\n\neval_tbl <- bind_rows(prueba_tbl, muestra_grande_tbl)\nresumen_vecinos_lim_tbl <- \n  predict(flujo_ajustado_vecinos_limite, eval_tbl) |> \n  mutate(predictor = \"vecinos_limite\") |> \n  bind_cols(eval_tbl) |> \n  group_by(predictor, tipo) |> \n  rmse(truth = y, estimate = .pred) \nresumen_lineal_lim_tbl <- \n  predict(flujo_ajustado_lineal_limite, eval_tbl) |> \n  mutate(predictor = \"lineal_limite\") |> \n  bind_cols(eval_tbl) |> \n  group_by(predictor, tipo) |> \n  rmse(truth = y, estimate = .pred) \nfmt_resumen(bind_rows(resumen_vmc_tbl, \n  resumen_lineal_tbl, \n  resumen_vecinos_lim_tbl, \n  resumen_lineal_lim_tbl) |> arrange(predictor))\n\n\n\n\n\n  \n  \n    \n      predictor\n      .metric\n      entrena\n      prueba\n    \n  \n  \n    lineal\nrmse\n49\n56\n    lineal_limite\nrmse\n54\n54\n    vecinos\nrmse\n36\n65\n    vecinos_limite\nrmse\n49\n49\n  \n  \n  \n\n\n\n\n¿Qué patrones ves en esta tabla? Podemos también graficar para entender mejor qué está pasando:\n\npreds_1 <- predict(flujo_ajustado_vecinos_limite, tibble(x = 0:17)) |> \n  bind_cols(tibble(x = 0:17, predictor = \"vecinos_limite\"))\npreds_2 <- predict(flujo_ajustado_lineal_limite, tibble(x = 0:17)) |> \n  bind_cols(tibble(x = 0:17, predictor = \"lineal_limite\"))\npreds_graf_tbl <- bind_rows(preds_1, preds_2, preds_graf_tbl) |> \n  mutate(predictor = factor(predictor))\ng_1 <- ggplot(datos_tbl, aes(x = x)) +\n  geom_line(data = preds_graf_tbl |> \n            filter(str_detect(predictor, \"vecinos|_óptimo\")), \n            aes(y = .pred, colour = predictor), size = 1.1) +\n  labs(subtitle = \"Vecinos\") \ng_2 <- ggplot(datos_tbl, aes(x = x)) +\n  geom_line(data = preds_graf_tbl |> \n            filter(str_detect(predictor, \"lineal|_óptimo\")), \n            aes(y = .pred, colour = predictor), size = 1.1) +\n  labs(subtitle = \"Lineal\") \ng_1 + g_2 + plot_layout(guides = 'collect')\n\n\n\n\nEsto patrón sugiere que:\n\nNuestro método de vecinos más cercanos no tiene errores por sesgo, sino más bien por sobreajuste o variabilidad.\nNuestro método lineal no tiene mucha variabilidad (el estimado con una muestra grande es casi igual al de la muestra de entrenamiento), sino más bien por sesgo\nEl error por sesgo se reduce usando métodos más flexibles o menos restringidos que puedan capturar patrones claros en los datos.\nPara reducir la variabilidad podemos usar métodos más simples o restringidos que no capturen tanto ruido.\nEl error irreducible se puede reducir incorporando información adicional relevante a las entradas.\n\n\n\n\n\n\n\nComplejidad y error de predicción\n\n\n\n\nMétodos de predicción más flexibles o complejos tienden a sufrir más de error por variabilidad, pues dependen fuertemente de la muestra utilizada.\nMétodos de predicción más rígidos o simples tienden a sufrir más error por sesgo, pues dependen menos de la muestra utilizada."
  },
  {
    "objectID": "02-principios-supervisado.html#agregando-más-información-y-error-irreducible",
    "href": "02-principios-supervisado.html#agregando-más-información-y-error-irreducible",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.6 Agregando más información y error irreducible",
    "text": "2.6 Agregando más información y error irreducible\nPodemos ver qué sucede cuando tenemos disponibles más variables relevantes. En este caso, probaremos con dos entradas:\n\ngenera_datos_2 <- function(n = 500, tipo = NULL){\n  dat_tbl <- tibble(nse = runif(n, 0, 100)) |>\n    mutate(estudio_años = floor(rnorm(n, 1.5 * sqrt(nse), 1))) |>\n    mutate(estudio_años = pmax(0, pmin(17, estudio_años))) |> \n    mutate(habilidad = rnorm(n, 100 + 0.1 * nse, 1)) |> \n    mutate(z = 100 + (habilidad/100) * ( 20 * nse + 5 * (estudio_años))) |> \n    mutate(ingreso = pmax(0, 0.2*(z + rnorm(n, 0, 150))))\n  obs_tbl <- dat_tbl |> \n    mutate(tipo = tipo, id = 1:n)\n  obs_tbl |> select(id, tipo, x_1 = estudio_años, x_2 = nse,  y = ingreso)\n}\n\n\nentrena_tbl <- genera_datos_2(20, tipo = \"entrena\")\nprueba_tbl <- genera_datos_2(500, tipo = \"prueba\")\nreceta_2 <- recipe(y ~ x_1 + x_2, data = entrena_tbl)\nflujo_lineal <- workflow() |> \n  add_recipe(receta_2) |> \n  add_model(modelo_lineal)\n# Ajustamos\nflujo_ajustado_lineal <- fit(flujo_lineal, entrena_tbl)\npredict(flujo_ajustado_lineal, bind_rows(entrena_tbl, prueba_tbl)) |> \n  bind_cols(bind_rows(entrena_tbl, prueba_tbl)) |>\n  group_by(tipo) |> \n  rmse(truth = y, estimate = .pred) |> gt() |> \n  fmt_number(.estimate, decimals = 1)\n\n\n\n\n\n  \n  \n    \n      tipo\n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    entrena\nrmse\nstandard\n27.1\n    prueba\nrmse\nstandard\n31.5\n  \n  \n  \n\n\n\n\nY vemos cómo inmediatamente redujimos el error de predicción: en este caso, aunque la variabilidad aumentó un poco (tenemos más parámetros que estimar vs el modelo con una sola variable), la reducción en el sesgo y en el error irreducible es tan grande que el desempeño es muy superior. Examina el caso de vecinos más cercanos."
  },
  {
    "objectID": "02-principios-supervisado.html#acerca-de-la-estimación-del-error-de-predicción",
    "href": "02-principios-supervisado.html#acerca-de-la-estimación-del-error-de-predicción",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.7 Acerca de la estimación del error de predicción",
    "text": "2.7 Acerca de la estimación del error de predicción\nCuando usamos una muestra de prueba limitada, podemos evaluar la precisión de nuestra estimación del error de predicción usando por ejemplo el bootstrap. En nuestro ejemplo anterior podríamos hacer los siguiente:\n\nlibrary(infer)\npreds <- predict(flujo_ajustado_lineal, bind_rows(prueba_tbl)) |> \n  bind_cols(prueba_tbl) \npreds |> \n  generate(reps = 1000, type = \"bootstrap\", variables = id) |> \n  group_by(replicate, tipo) |> \n  rmse(truth = y, estimate = .pred) |> \n  select(replicate, tipo, stat = .estimate) |>\n  get_ci(level = 0.90) |> \n  gt() |> fmt_number(where(is_double), decimals = 1)\n\nWarning: The `variables` argument is only relevant for the \"permute\" generation\ntype and will be ignored.\n\n\n\n\n\n\n  \n  \n    \n      lower_ci\n      upper_ci\n    \n  \n  \n    29.9\n33.0"
  },
  {
    "objectID": "02-principios-supervisado.html#resumen",
    "href": "02-principios-supervisado.html#resumen",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.8 Resumen",
    "text": "2.8 Resumen\n\n\n\n\n\n\nTarea fundamental del análisis supervisado\n\n\n\n\nUsando datos de entrenamiento \\({\\mathcal L}\\), construimos una funcion \\(\\hat{f}\\) para predecir. Estas funciones se ajustan usualmente intentando estimar directamente el predictor óptimo \\(f^*(x)\\) (si lo conocemos teóricamente), o indirectamente intentando minimizar la pérdida sobre el conjunto de entrenamiento.\nSi observamos nuevos valores \\(\\mathbf{x}\\), nuestra predicción es \\(\\hat{y} = \\hat{f}(\\mathbf{x})\\).\nBuscamos que cuando observemos nuevos casos para predecir, nuestro error de predicción sea bajo en promedio (\\(Err\\) sea bajo).\nUsualmente estimamos \\(Err\\) mediante una muestra de prueba o validación \\({\\mathcal T}\\).\nNos interesan métodos de construir \\(\\hat{f}\\) que produzcan errores de predicción bajos.\n\n\n\n\nNótese que el error de entrenamiento se calcula sobre la muestra \\({\\mathcal L}\\) que se usó para construir \\(\\hat{f}\\), mientras que el error de predicción se estima usando una muestra independiente \\({\\mathcal T}\\).\n\\(\\hat{Err}\\) es una estimación razonable de el error de predicción \\(Err\\) (por ejemplo, \\(\\hat{Err} \\to Err\\) cuando el tamaño de la muestra de prueba crece), pero \\(\\overline{err}\\) típicamente es una estimación mala del error de predicción.\n\n\n\n\n\n\n\nReduciendo el error de predicción\n\n\n\nPara reducir el error de predicción, podemos:\n\nIncluir variables relevantes que reduzcan el error irreducible\nReducir variabilidad usando métodos más estables o menos complejos\nReducir sesgo usando métodos más flexibles\nUsar métodos con la estructura adecuada para el problema\n\nGeneralmente 2 y 3 están en contraposición, a lo que muchas veces se le llama equilibrio de varianza y sesgo. Los puntos 1 y 4 generalmente mejoran los resultados reduciendo tanto sesgo como variabilidad."
  },
  {
    "objectID": "02-principios-supervisado.html#qué-cosas-no-veremos-en-este-curso",
    "href": "02-principios-supervisado.html#qué-cosas-no-veremos-en-este-curso",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.9 Qué cosas no veremos en este curso",
    "text": "2.9 Qué cosas no veremos en este curso\nEn este curso nos concentraremos en la construcción, evaluación y mejora de modelos predictivos. Para que estas ideas funcionen en problemas reales, hay más aspectos a considerar que no discutiremos con detalle (y muchas veces son considerablemente más difíciles de la teoría y los algoritmos):\n\nPara entender exactamente cuál es el problema que queremos resolver se requiere trabajo analítico considerable, y también trabajo en entender aspectos del área o negocio donde nos interesa usar aprendizaje máquina. Muchas veces es fácil resolver un problema muy preciso, que tenemos a la mano, pero que más adelante nos damos cuenta de que no es útil.\nEstos dos puntos incluyen indentificar las métricas que queremos mejorar, lo cual no siempre se claro. Optimizar métricas incorrectas es poco útil en el mejor de los casos, y en los peores pueden causar daños. Evitar esto requiere monitoreo constante de varios aspectos del funcionamiento de nuestros modelos y sus consecuencias.\n¿Cómo poner en producción modelos y mantenerlos? Un flujo apropiado de trabajo, y de entrenamiento continuo puede ser la diferencia entre entre un modelo exitoso o uno que se vuelve fuente de dificultades y confusión."
  },
  {
    "objectID": "03-metodos-locales.html",
    "href": "03-metodos-locales.html",
    "title": "3  Métodos locales no estructurados",
    "section": "",
    "text": "De la discusión de la sección anterior, y examinando el método de \\(k\\) vecinos más cercanos, puede dar la impresión de que si tenemos suficientes datos, métodos locales como \\(k\\) vecinos pueden ser superiores a otros métodos más estructurados como regresión lineal, que necesariamente incurren en sesgo porque su estructura siempre está mal especificada.\nSin embargo, no es necesario que se cumpla exactamente el supuesto lineal para que los predictores lineales funcionen, y veremos que en casos típicos los métodos locales como \\(k\\)-vecinos más cercanos rara vez funcionan apropiadamente."
  },
  {
    "objectID": "03-metodos-locales.html#controlando-complejidad",
    "href": "03-metodos-locales.html#controlando-complejidad",
    "title": "3  Métodos locales no estructurados",
    "section": "3.1 Controlando complejidad",
    "text": "3.1 Controlando complejidad\nPrimero examinamos cómo controlamos el nivel de complejidad para un método local como \\(k\\) vecinos más cercanos. La idea es que:\n\nMás complejidad: Si tomamos \\(k\\) demasiado chica, cada estimación usa pocos datos y puede ser ruidosa (incurrimos en variabilidad)\nMenos complejidad: Si tomamos \\(k\\) demasiado grande, cada estimación usa potencialmente datos no relevantes muy lejanos a donde queremos predecir (incurrimos en sesgo)\n\nComenzamos con un ejemplo simple en dimensión baja:\n\nEjemplo\n\nlibrary(tidyverse)\nlibrary(gt)\nauto <- read_csv(\"../datos/auto.csv\")\n# seleccionar variables y poner en sistema métrico\ndatos <- auto |> \n  select(name, weight, year, mpg, displacement) |> \n  mutate(\n    peso_kg = weight * 0.45359237,\n    rendimiento_kpl = mpg * (1.609344 / 3.78541178), \n    año = year\n  )\n\nVamos a separa en muestra de entrenamiento y de prueba estos datos. Podemos hacerlo como sigue (75% para entrenamiento aproximadamente en este caso, así obtenemos alrededor de 100 casos para prueba):\n\nlibrary(tidymodels)\nset.seed(121)\ndatos_split <- initial_split(datos, prop = 0.75)\ndatos_entrena <- training(datos_split)\ndatos_prueba <- testing(datos_split)\nnrow(datos_entrena)\n\n[1] 294\n\nnrow(datos_prueba)\n\n[1] 98\n\n\nVamos a usar año y peso de los coches para predecir su rendimiento:\n\nggplot(datos_entrena, \n  aes(x = peso_kg, y = rendimiento_kpl, colour = año)) +\n  geom_point()\n\n\n\n\nProbaremos con varios valores para \\(k\\), el número de vecinos más cercanos. La función de predicción ajustada es entonces:\n\n# nótese que normalizamos entradas - esto también es importante\n# hacer cuando hacemos vecinos más cercanos, pues en otro caso\n# las variables con escalas más grandes dominan el cálculo\nvmc_1 <- nearest_neighbor(neighbors = tune(), weight_func = \"gaussian\") |>  \n  set_engine(\"kknn\") |>  \n  set_mode(\"regression\")\nreceta_vmc <- recipe(\n  rendimiento_kpl ~ peso_kg + año, datos_entrena) |> \n  step_normalize(all_predictors()) \nflujo_vecinos <- workflow() |>  \n  add_recipe(receta_vmc) |> \n  add_model(vmc_1)\n# definir parámetros que nos interesa explorar\nvecinos_params <- parameters(neighbors(range = c(1, 100)))\n# definir cuáles valores de los parámetros exploramos\nvecinos_grid <- grid_regular(vecinos_params, levels = 100)\nmis_metricas <- metric_set(rmse)\n\nEn la siguiente gráfica mostramos cómo cambia el error de los las predicciones sobre la muestra de prueba separada de la de entrenamiento. En este caso le llamaremos muestra de validación porque más adelante veremos que puede ser conveniente dividir en entrenamiento-validación-prueba en lugar de usar sólo 2 particiones:\n\nr_split <- manual_rset(list(datos_split), \"validación\")\nvecinos_eval_tbl <- tune_grid(flujo_vecinos,\n                            resamples = r_split,\n                            grid = vecinos_grid,\n                            metrics = mis_metricas) \nvecinos_ajustes_tbl <- vecinos_eval_tbl |>\n  unnest(cols = c(.metrics)) |> \n  select(id, neighbors, .metric, .estimate)\nggplot(vecinos_ajustes_tbl, aes(x = neighbors, y = .estimate)) +\n  geom_line() + geom_point() +\n  ylab(\"Error de validación\") + xlab(\"Vecinos\")\n\n\n\n\nDonde obtenemos más o menos lo que esperaríamos: modelos con muy pocos vecinos o demasiados vecinos se desempeñan relativamente mal.\nSeleccionaremos el mejor modelo según el error estimado de predicción y visualizamos primero nuestras predicciones y los datos de entrenamiento de la siguiente forma:\n\nmejor_rmse <- select_best(vecinos_eval_tbl, metric = \"rmse\")\najuste_1 <- finalize_workflow(flujo_vecinos, mejor_rmse) |> \n  fit(datos_entrena)\ndat_graf <- tibble(peso_kg = seq(900, 2200, by = 10)) |> \n  crossing(tibble(año = c(70, 75, 80)))\ndat_graf <- dat_graf |> \n  mutate(pred_1 = predict(ajuste_1, dat_graf) |> pull(.pred))\nggplot(datos_entrena, aes(x = peso_kg, group = año, colour = año)) +\n  geom_point(aes(y = rendimiento_kpl), alpha = 0.6) + \n  geom_line(data = dat_graf, aes(y = pred_1),  size = 1.2)\n\n\n\n\nEl método parece funcionar razonablemente bien para este problema simple. Sin embargo, si el espacio de entradas no es de dimensión baja, entonces podemos encontrarnos con dificultades."
  },
  {
    "objectID": "03-metodos-locales.html#la-maldición-de-la-dimensionalidad",
    "href": "03-metodos-locales.html#la-maldición-de-la-dimensionalidad",
    "title": "3  Métodos locales no estructurados",
    "section": "3.2 La maldición de la dimensionalidad",
    "text": "3.2 La maldición de la dimensionalidad\nEl método de k-vecinos más cercanos funciona mejor cuando\n\nNo es necesario hacer \\(k\\) demasiado grande, de forma que terminemos tomando valores lejanos que inducen sesgo.\nNo es necesario hacer \\(k\\) demasiado chica, de forma que nuestras predicciones sean inestables.\n\n\n\n\n\n\n\nMaldición de la dimensionalidad\n\n\n\nEn dimensión alta, para la mayoría de las \\(\\mathbf{x}\\) donde queremos hacer predicciones típicamente no existen vecinos cercanos, aún para conjuntos de entrenamiento muy grandes.\n\n\nEsto implica que para tamaños típicos \\(n\\) de muestra de entrenamiento:\n\nSi tomamos \\(k\\) chica, el sesgo por especificación es chico (con muestras muy grandes), pero el sesgo de estimación puede ser grande pues estamos de todas formas obigados a buscar vecinos lejos de donde queremos predecir.\nSi tomamos \\(k\\) más grande, podemos quizá reducir sesgo de estimación, pero entonces el sesgo por especificación puede ser más grande (pues promediamos sobre regiones relativamente grandes), y perdemos la supuesta ventaja del método local.\nPara que una \\(k\\) chica tenga sesgo de estimación bajo, el tamaño \\(n\\) de la muestra de entrenamiento tiene que ser gigantesca.\n\n\nEjemplo\nConsideremos que la salida Y es determinística \\(Y = e^{-8\\sum_{j=1}^p x_j^2}\\). Vamos a usar 1-vecino más cercano para hacer predicciones, con una muestra de entrenamiento de 1000 casos. Generamos $x^{i}’s uniformes en \\([ 1,1]\\), para \\(p = 2\\), y calculamos la respuesta \\(Y\\) para cada caso:\n\nfun_exp <- function(x) exp(-8 * sum(x ^ 2))\nx <- map(1:1000, ~ runif(2, -1, 1))\ndat <- tibble(x = x) |> \n        mutate(y = map_dbl(x, fun_exp))\nggplot(dat |> mutate(x_1 = map_dbl(x, 1), x_2 = map_dbl(x, 2)), \n       aes(x = x_1, y = x_2, colour = y)) + geom_point()\n\n\n\n\nLa mejor predicción en \\(x_0 = (0,0)\\) es \\(f((0,0)) = 1\\). El vecino más cercano al origen es\n\ndat <- dat |> mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) |> \n  arrange(dist_origen)\nmas_cercano <- dat[1, ]\nmas_cercano\n\n# A tibble: 1 × 3\n  x             y dist_origen\n  <list>    <dbl>       <dbl>\n1 <dbl [2]> 0.995      0.0261\n\nmas_cercano$x[[1]]\n\n[1] -0.025090354  0.007277334\n\n\nNuestra predicción es entonces \\(\\hat{f}(0)=\\) 0.994555, que es bastante cercano al valor verdadero (1).\nAhora intentamos hacer lo mismo para dimensión \\(p=8\\).\n\nx <- map(1:1000, ~ runif(8, -1, 1))\ndat <- tibble(x = x) |> \n       mutate(y = map_dbl(x, fun_exp))\ndat <- dat |> mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) |> \n  arrange(dist_origen)\nmas_cercano <- dat[1, ]\nmas_cercano\n\n# A tibble: 1 × 3\n  x             y dist_origen\n  <list>    <dbl>       <dbl>\n1 <dbl [8]> 0.104       0.532\n\nmas_cercano$x[[1]]\n\n[1]  0.30027994  0.36774993 -0.06613864 -0.03673154  0.12260975  0.16718980\n[7] -0.01866598 -0.09308947\n\n\nY el resultado es un desastre. Nuestra predicción es\n\nmas_cercano$y\n\n[1] 0.1038249\n\n\nNecesitariamos una muestra de alrededor de un millón de casos para obtener resultados no tan malos (haz pruebas).\n¿Qué es lo que está pasando? La razón es que en dimensiones altas, los puntos de la muestra de entrenamiento están muy lejos unos de otros, y están cerca de la frontera, incluso para tamaños de muestra relativamente grandes como n = 1000. Cuando la dimensión crece, la situación empeora exponencialmente."
  },
  {
    "objectID": "03-metodos-locales.html#regresión-lineal-en-dimensión-alta",
    "href": "03-metodos-locales.html#regresión-lineal-en-dimensión-alta",
    "title": "3  Métodos locales no estructurados",
    "section": "3.3 Regresión lineal en dimensión alta",
    "text": "3.3 Regresión lineal en dimensión alta\nAhora intentamos algo similar con una función que es razonable aproximar con una función lineal:\n\nfun_cuad <- function(x)  0.5 * (1 + x[1])^2\n\nY queremos predecir para \\(x=(0,0,\\ldots,0)\\), cuyo valor exacto es\n\nfun_cuad(0)\n\n[1] 0.5\n\n\nLos datos se generan de la siguiente forma:\n\nsimular_datos <- function(p = 40){\n    x <- map(1:1000,  ~ runif(p, -1, 1))\n    dat <- tibble(x = x) |> mutate(y = map_dbl(x, fun_cuad)) \n    dat\n}\n\nPor ejemplo para dimensión baja \\(p=1\\) (nótese que una aproximación lineal es razonable):\n\nejemplo <- simular_datos(p = 1) |> mutate(x = unlist(x))\nggplot(ejemplo, aes(x = x, y = y)) + geom_point() +\n    geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nAhora repetimos el proceso en dimensión \\(p=40\\): simulamos las entradas, y aplicamos un vecino más cercano\n\nvmc_1 <- function(dat){\n    dat <- dat |> \n        mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) |> \n        arrange(dist_origen)\n        mas_cercano <- dat[1, ]\n        mas_cercano$y\n}\nset.seed(834)\ndat <- simular_datos(p = 40)\nvmc_1(dat)\n\n[1] 1.206478\n\n\nEste no es un resultado muy bueno. Sin embargo, regresión se desempeña considerablemente mejor:\n\nregresion_pred <- function(dat){\n    p <- length(dat$x[[1]])\n    dat_reg <- cbind(\n        y = dat$y, \n        x = matrix(unlist(dat$x), ncol = p, byrow=T)) |> \n        as.data.frame()\n    mod_lineal <- lm(y ~ ., dat = dat_reg)\n    origen <- data.frame(matrix(rep(0, p), 1, p))\n    names(origen) <- names(dat_reg)[2:(p+1)]\n    predict(mod_lineal, newdata = origen)\n}\nregresion_pred(dat)\n\n        1 \n0.6677861 \n\n\nLa razón de este mejor desempeño de regresión es que en este caso, el modelo lineal explota la estructura aproximadamente lineal del problema (¿cuál estructura lineal? haz algunas gráficas). Nota: corre este ejemplo varias veces con semilla diferente.\nSolución: vamos a hacer varias simulaciones, para ver qué modelo se desempeña mejor.\n\nsims <- map(1:200, function(i){\n    dat <- simular_datos(p = 40)\n    vmc_y <- vmc_1(dat)\n    reg_y <- regresion_pred(dat)\n    tibble(rep = i, \n           error = c(abs(vmc_y - 0.5), abs(reg_y - 0.5)), \n            tipo = c(\"vmc\", \"regresion\"))\n}) |> bind_rows()\nggplot(sims, aes(x = tipo, y = error)) + geom_boxplot() \n\n\n\n\nAsí que típicamente el error de vecinos más cercanos es más alto que el de regresión. El error esperado es para vmc es más de doble que el de regresión:\n\nsims |> group_by(tipo) |> \n  summarise(media_error = mean(error)) |> \n  gt()\n\n\n\n\n\n  \n  \n    \n      tipo\n      media_error\n    \n  \n  \n    regresion\n0.1662124\n    vmc\n0.3542532\n  \n  \n  \n\n\n\n\nLo que sucede más específicamente es que en regresión lineal utilizamos todos los datos para hacer nuestra estimación en cada predicción. Si la estructura del problema es aproximadamente lineal, entonces regresión lineal explota la estructura para hacer pooling de toda la información para construir predicción con sesgo y varianza bajas. En contraste, vecinos más cercanos sufre de varianza alta.\n\n\n\n\n\n\nMétodos locales sin estructura\n\n\n\nLos métodos locales muchas veces no funcionan bien en dimensión alta. La razón es que:\n\nEl sesgo es alto, pues promediamos puntos muy lejanos al lugar donde queremos predecir (aunque tomemos pocos vecinos cercanos).\nEn el caso de que encontremos unos pocos puntos cercanos, la varianza también puede ser alta porque promediamos relativamente pocos vecinos.\n\nMétodos con más estructura global, apropiada para el problema, logran explotar apropiadamente información de puntos que no están tan cerca del lugar donde queremos predecir.\n\n\nMuchas veces el éxito en la predicción depende de establecer esas estructuras apropiadas (por ejemplo, efectos lineales cuando variables tienen efectos aproximadamente lineales, árboles cuando hay algunas interacciones, redes convolucionales para procesamiento de imágenes y señales, dependencia del contexto inmediato en modelos de lenguaje, etc.)"
  },
  {
    "objectID": "04-lineales-ingenieria.html",
    "href": "04-lineales-ingenieria.html",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "",
    "text": "Una de las maneras más simples que podemos intentar para predecir \\(y\\) en función de las \\(x_j\\)´s es mediante una suma ponderada de los valores de las \\(x_j's\\), usando una función de la forma\n\\[f_\\beta (x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p,\\]\nEn este caso, escoger una función particular de esta familia, dada una muestra de entrenamiento \\({\\mathcal L}\\), consiste en encontrar encontrar valores apropiados de las \\(\\beta\\)’s, para construir un predictor:\n\\[\\hat{f}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 \\cdots + \\hat{\\beta} x_p\\]\ny usaremos esta función \\(\\hat{f}\\) para hacer predicciones \\(\\hat{y} =\\hat{f}(x)\\)."
  },
  {
    "objectID": "04-lineales-ingenieria.html#aprendizaje-de-coeficientes-ajuste",
    "href": "04-lineales-ingenieria.html#aprendizaje-de-coeficientes-ajuste",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.1 Aprendizaje de coeficientes (ajuste)",
    "text": "4.1 Aprendizaje de coeficientes (ajuste)\nEn el ejemplo anterior, los coeficientes fueron calculados (o estimados) usando experiencia, reglas, argumentos teóricos, o quizá otras fuentes de datos (como estudios o encuestas, conteos, etc.)\nAhora quisiéramos construir un algoritmo para aprender estos coeficientes del modelo\n\\[f_\\beta (x) = \\beta_0 + \\beta_1 x_1 + \\cdots \\beta_p x_p\\]\na partir de una muestra de entrenamiento de datos históricos de tiendas que hemos abierto antes:\n\\[{\\mathcal L}=\\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}\\]\nEl criterio de ajuste (algoritmo de aprendizaje) más usual para regresión lineal es el de mínimos cuadrados.\nConstruimos las predicciones (ajustados) para la muestra de entrenamiento:\n\\[ f_\\beta (x^{(i)}) = \\beta_0 + \\beta_1 x_1^{(i)}+ \\cdots + \\beta_p x_p^{(i)}\\]\nY consideramos las diferencias de los ajustados con los valores observados:\n\\[e^{(i)} = y^{(i)} - f_\\beta (x^{(i)})\\]\nLa idea entonces es minimizar la suma de los residuales al cuadrado, para intentar que la función ajustada pase lo más cercana a los puntos de entrenamiento que sea posible. La función de pérdida que utilizamos más frecuentemente es la pérdida cuadrática, dada por:\n\\[L(\\beta) = \\frac{1}{N}\\sum_{i=1}^N (y^{(i)} - f_\\beta(x^{(i)}))^2\\]\n\n\n\n\n\n\nMínimos cuadrados para regresión lineal\n\n\n\nBuscamos encontrar:\n\\[\\hat{\\beta} = \\mathrm{arg\\,min}_{\\beta} L(\\beta) = \\mathrm{arg\\,min}_{\\beta}\\frac{1}{N}\\sum_{i=1}^N (y^{(i)} - f_\\beta(x^{(i)}))^2\\]\ndonde\n\\[f_\\beta (x^{(i)}) = \\beta_0 + \\beta_1 x_1^{(i)}+ \\cdots + \\beta_p x_p^{(i)}\\]\n\n\nHay varias maneras de resolver este problema: puede hacerse analíticamente con álgebra lineal, o con algún método numérico como descenso máximo (que puede escalarse fácilmente). Típicamente la función objetivo es convexa, y la solución es única, excepto en casos degenerados que podremos evitar más adelante usando regularización.\nObservación: Como discutimos al final de la sección anterior, minimizar directamente el error de entrenamiento para encontrar los coeficientes puede resultar en en un modelo sobreajustado/con varianza alta/ruidoso. Hay tres grandes estrategias para mitigar este problema: restringir o estructurar la familia de funciones, penalizar la función objetivo o perturbar la muestra de entrenamiento. El método mas común es cambiar la función objetivo, que discutiremos más adelante en la sección de regularización."
  },
  {
    "objectID": "04-lineales-ingenieria.html#ingeniería-de-entradas",
    "href": "04-lineales-ingenieria.html#ingeniería-de-entradas",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.2 Ingeniería de entradas",
    "text": "4.2 Ingeniería de entradas\nAlgunas veces, encontrar la estructura apropiada puede requerir más trabajo que simplemente escoger una familia de modelos. Por ejemplo, en el caso de precios de casa, vimos que podríamos mejorar el ajuste haciendo que el coeficiente de área habitable dependiera de la calidad de los terminados @ref(medicioncostosa).\nUsualmente tendremos que hacer varias transformaciones para obtener buen desempeño de un modelo lineal. En la siguientes secciones mostramos algunas de las más usuales."
  },
  {
    "objectID": "04-lineales-ingenieria.html#variables-categóricas",
    "href": "04-lineales-ingenieria.html#variables-categóricas",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.3 Variables categóricas",
    "text": "4.3 Variables categóricas\nEn primer lugar, podemos incluir variables categóricas creando variables numéricas 0-1 para cada categoría. Por ejemplo para la variable calidad sótano:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gt)\nsource(\"../R/casas_traducir_geo.R\")\nset.seed(83)\ncasas_split <- initial_split(casas, prop = 0.75)\ncasas_entrena <- training(casas_split)\ncasas_entrena |> count(calidad_sotano)\n\n# A tibble: 5 × 2\n  calidad_sotano     n\n  <chr>          <int>\n1 Ex                89\n2 Fa                23\n3 Gd               457\n4 TA               500\n5 <NA>              26\n\n\nEl mejor nivel es Ex (excelente), luego sigue Gd (bueno), luego Fa (razonable) y finalmente TA (típico)). Hay otro nivel Po (Malo) que no aparece en estos datos.\nEn primer lugar, podemos codificar los valores faltantes, que en este caso indican casas sin sótano:\n\nreceta_na <- recipe(~ calidad_sotano, casas_entrena) |> \n  step_unknown(calidad_sotano, new_level = \"no_sótano\") |> \n  step_relevel(calidad_sotano, ref_level = \"TA\")\ncasas_preproc <- prep(receta_na) |> juice()\ncasas_preproc |> count(calidad_sotano)\n\n# A tibble: 5 × 2\n  calidad_sotano     n\n  <fct>          <int>\n1 TA               500\n2 Ex                89\n3 Fa                23\n4 Gd               457\n5 no_sótano         26\n\n\nAhora convertimos a codificación dummy:\n\nset.seed(7)\nreceta_dummy <- \n  recipe( ~ calidad_sotano, casas_entrena) |> \n  step_unknown(calidad_sotano, new_level = \"no_sótano\") |> \n  step_relevel(calidad_sotano, ref_level = \"TA\") |> \n  step_dummy(calidad_sotano, keep_original_cols = TRUE)\n# preparar receta\nreceta_dummy_prep <- prep(receta_dummy) \n# extrae los datos de entrenamiento preprocesados\nreceta_dummy_prep |> juice() |> \n  sample_n(10) |> gt() |> \n  tab_options(table.font.size = 10)\n\n\n\n\n\n  \n  \n    \n      calidad_sotano\n      calidad_sotano_Ex\n      calidad_sotano_Fa\n      calidad_sotano_Gd\n      calidad_sotano_no_sótano\n    \n  \n  \n    TA\n0\n0\n0\n0\n    Gd\n0\n0\n1\n0\n    Ex\n1\n0\n0\n0\n    Ex\n1\n0\n0\n0\n    TA\n0\n0\n0\n0\n    TA\n0\n0\n0\n0\n    TA\n0\n0\n0\n0\n    TA\n0\n0\n0\n0\n    TA\n0\n0\n0\n0\n    TA\n0\n0\n0\n0\n  \n  \n  \n\n\n\n\nNótese que no hay columna para el nivel TA, que tomamos como referencia. Incluir esta columna sería redundante, pues tenemos una constante en el predictor. En general, cuando una variable categórica tiene \\(k\\) niveles, esta codificación produce \\(k-1\\) columnas binarias.\nVeamos qué pasa cuando preprocesamos datos de prueba (para después poder hacer predicciones):\n\nprueba_casas <- testing(casas_split)\n# supongamos que un nuevo nivel aparece\nprueba_casas$calidad_sotano[1] <- \"no visto antes\"\ndatos <- bake(receta_dummy_prep, prueba_casas)\n\nWarning: There are new levels in a factor: no visto antes\nNew levels will be coerced to `NA` by `step_unknown()`.\nConsider using `step_novel()` before `step_unknown()`.\n\n\nWarning: There are new levels in a factor: NA\n\n\nEn este caso, podemos hacer nuestro flujo más robusto incluyendo un nuevo nivel en los factores donde pondremos casos no vistos. Modificamos nuestra receta:\n\nreceta_dummy <- \n  recipe( ~ calidad_sotano, casas_entrena) |> \n  step_novel(calidad_sotano, new_level = \"nuevo\") |> \n  step_unknown(calidad_sotano, new_level = \"no_sótano\") |> \n  step_relevel(calidad_sotano, ref_level = \"TA\") |> \n  step_dummy(calidad_sotano, keep_original_cols = TRUE)\n# preparar receta\nreceta_dummy_prep <- prep(receta_dummy) \n\n\nprueba_casas <- testing(casas_split)\n# supongamos que un nuevo nivel aparece\nprueba_casas$calidad_sotano[1] <- \"no visto antes\"\ndatos <- bake(receta_dummy_prep, prueba_casas)\ndatos |> head() |> gt() |> tab_options(table.font.size = 10)\n\n\n\n\n\n  \n  \n    \n      calidad_sotano\n      calidad_sotano_Ex\n      calidad_sotano_Fa\n      calidad_sotano_Gd\n      calidad_sotano_nuevo\n      calidad_sotano_no_sótano\n    \n  \n  \n    nuevo\n0\n0\n0\n1\n0\n    Ex\n1\n0\n0\n0\n0\n    TA\n0\n0\n0\n0\n0\n    Gd\n0\n0\n1\n0\n0\n    TA\n0\n0\n0\n0\n0\n    TA\n0\n0\n0\n0\n0\n  \n  \n  \n\n\n\n\nY podemos ignorar el nuevo nivel al hacer predicciones (que equivale a ponerlo en la categoría de referencia, que en este caso es TA), o podemos lidiar de manera ad-hoc con este nivel.\nOtro problema con el que podemos encontrarnos es variables categóricas que son muy ralas. Por ejemplo, una variable que tiene muchas categorías y algunas de ellas tienen muy pocos datos, además de que es probable que observemos nuevas categorías en el futuro. Por ejemplo, para la variable de zona:\n\ncasas_entrena |> count(nombre_zona) |> \n  arrange(desc(n)) |> gt() |> tab_options(table.font.size = 10)\n\n\n\n\n\n  \n  \n    \n      nombre_zona\n      n\n    \n  \n  \n    NAmes\n163\n    CollgCr\n113\n    OldTown\n80\n    Edwards\n74\n    Somerst\n65\n    Gilbert\n61\n    Sawyer\n59\n    NridgHt\n58\n    NWAmes\n57\n    BrkSide\n45\n    SawyerW\n42\n    Crawfor\n39\n    Mitchel\n36\n    IDOTRR\n31\n    NoRidge\n30\n    Timber\n26\n    ClearCr\n22\n    StoneBr\n20\n    SWISU\n18\n    Blmngtn\n14\n    BrDale\n14\n    MeadowV\n12\n    NPkVill\n8\n    Veenker\n7\n    Blueste\n1\n  \n  \n  \n\n\n\n\nEn este caso, tenemos muchas categorías, algunas con muy pocos datos, y es posible que observemos nuevos datos. Una técnica es agrupar los datos de baja cardinalidad en un nuevo nivel (incluyendo categorías no observadas en entrenamiento):\n\nreceta_vecindario_1 <- \n  recipe( ~ nombre_zona, casas_entrena) |>\n  step_other(nombre_zona, threshold = 0.01, other = \"otras\") \nreceta_vecindario <- receta_vecindario_1 |> \n  step_dummy(nombre_zona)\n# preparar receta\nreceta_vecindario_prep <- prep(receta_vecindario_1)\nset.seed(8231)\nreceta_vecindario_prep |> juice() |> \n  count(nombre_zona) |> arrange(desc(n)) |> gt()\n\n\n\n\n\n  \n  \n    \n      nombre_zona\n      n\n    \n  \n  \n    NAmes\n163\n    CollgCr\n113\n    OldTown\n80\n    Edwards\n74\n    Somerst\n65\n    Gilbert\n61\n    Sawyer\n59\n    NridgHt\n58\n    NWAmes\n57\n    BrkSide\n45\n    SawyerW\n42\n    Crawfor\n39\n    Mitchel\n36\n    IDOTRR\n31\n    NoRidge\n30\n    Timber\n26\n    ClearCr\n22\n    StoneBr\n20\n    SWISU\n18\n    otras\n16\n    Blmngtn\n14\n    BrDale\n14\n    MeadowV\n12\n  \n  \n  \n\n\n\n\nEn este caso, las zonas de baja frecuencia fueron agrupadas en la categoría “otras”. Si observamos un nuevo nivel al momento de predicción:\n\nprueba_casas <- testing(casas_split)\n# supongamos que un nuevo nivel aparece\nprueba_casas$nombre_zona[1] <- \"Xochimilco\"\ndatos <- bake(prep(receta_vecindario), prueba_casas)\ndatos |> head() |>  gt() |> tab_options(table.font.size = 10)\n\n\n\n\n\n  \n  \n    \n      nombre_zona_BrDale\n      nombre_zona_BrkSide\n      nombre_zona_ClearCr\n      nombre_zona_CollgCr\n      nombre_zona_Crawfor\n      nombre_zona_Edwards\n      nombre_zona_Gilbert\n      nombre_zona_IDOTRR\n      nombre_zona_MeadowV\n      nombre_zona_Mitchel\n      nombre_zona_NAmes\n      nombre_zona_NoRidge\n      nombre_zona_NridgHt\n      nombre_zona_NWAmes\n      nombre_zona_OldTown\n      nombre_zona_Sawyer\n      nombre_zona_SawyerW\n      nombre_zona_Somerst\n      nombre_zona_StoneBr\n      nombre_zona_SWISU\n      nombre_zona_Timber\n      nombre_zona_otras\n    \n  \n  \n    0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n    0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n    0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n    0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n    0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n    0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n  \n  \n  \n\n\n\n\nEl proceso general es (ver por ejemplo esta lista):\n\n\n\n\n\n\nVariables categóricas\n\n\n\n\nEstablecemos los niveles que puede tener cada variable, incluyendo la posibilidad de categorías nuevas al momento de predecir, y categorías para valores no disponibles (NAs) (es posible también imputar con algún método en caso necesario).\nReorganizamos factores dependiendo del problema. Por ejemplo, incluir categorías de baja frecuencia en una categoría separada, o manipulaciones ad-hoc dependiendo del problema.\nSustituimos variables categóricas con \\(K\\) niveles en \\(K-1\\) columnas indicadoras de los niveles (estableciendo) alguna categoría como referencia."
  },
  {
    "objectID": "04-lineales-ingenieria.html#interacciones",
    "href": "04-lineales-ingenieria.html#interacciones",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.4 Interacciones",
    "text": "4.4 Interacciones\nOtra manera de expandir nuestro modelo es la utilización de interacciones, que muchas veces son clave para tener éxito con modelos lineales. Vimos ejemplos de interacciones en el ejemplo de las casas (@ref(medicioncostosa)) y en el primer ejemplo de ventas de tiendas que dependían del tráfico.\n\nEjemplo\nSi \\(x_1\\) es el área en metros cuadrados de una casa, y \\(x_2\\) una calificación numérica de su calidad, podemos considerar el modelo sin interacciones:\n\\[\\beta_0 + \\beta_1x_1 + \\beta_2 x_2\\]\nPero no tiene mucho sentido que el efecto marginal de \\(x_1\\) sea constante para cualquier nivel de calidad, y tampoco que la calidad de terminados agregue una cantidad fija al precio de la casa sin tomar en cuenta su tamaño. Podemos remediar esto creando una nueva variable que es le producto de \\(x_1\\) y \\(x_2\\):\n\\[x_3 = x_1 x_2\\]\ny agregando, nuestro predictor para precio es\n\\[ \\beta_0 +  \\beta_1x_1 + \\beta_2 x_2 + \\beta_3 x_1x_2\\]\nAhora notemos que para \\(x_2\\) fija, el modelo es\n\\[(\\beta_0 + \\beta_2x_2) + (\\beta_1 + \\beta_3x_2)x_1 = \\gamma_0 + \\gamma_1x_1\\]\nDe modo que es lineal en \\(x_1\\). La diferencia es que cuando cambia \\(x_2\\), la recta que ajustamos es diferente.\n\nbeta <- c(0, 50, 100, 20)\ncombs_tbl <- crossing(x_1 = seq(2, 20, by = 1), x_2 = seq(0, 10, by = 2)) |> \n  mutate(x_3 = x_1 * x_2) |> \n  mutate(pred = beta[1] + beta[2]*x_1 + beta[3]*x_2 + beta[4]*x_3)\nggplot(combs_tbl, aes(x = x_1, y = pred, group = x_2, colour = x_2)) +\n  geom_line()\n\n\n\n\nY vemos que cuando la calidad es baja, el precio por metro cuadrado es más bajo que cuando la calidad es alta. Otra manera de pensar esto es que la inclusión de la interacción produce curvas marginales que rotan dependiendo del valor de otras variables.\nPregunta. ¿puedes pensar en otros casos donde las interacciones deben jugar un papel importante?\n\n\n\n\n\n\nInteracciones\n\n\n\n\nTransformamos las variables categóricas a dummies. Transformamos las variables numéricas si es necesario (normalizar, aplicar transformación no lineal, etc.)\nIncluimos interacciones de la siguiente forma:\n\n\nPara la interaccion de dos variables numéricas \\(x_1\\) y \\(x_2\\) agregamos el producto \\(x_3 = x_1x_2\\).\nPara interacción de una variable categórica \\(g\\) con una numérica \\(x\\) podemos hacer el mismo procedimiento multiplicando la variable categórica por cada una de las variable dummy que creamos a partir de \\(g\\). Esto en efecto produce una pendiente para \\(x\\) dependiendo del valor que toma \\(g\\)."
  },
  {
    "objectID": "04-lineales-ingenieria.html#ejemplo-precios-de-casas",
    "href": "04-lineales-ingenieria.html#ejemplo-precios-de-casas",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.5 Ejemplo: precios de casas",
    "text": "4.5 Ejemplo: precios de casas\nEn el ejemplo de precios de casas, por ejemplo, es claro que el efecto en ventas del tamaño de las áreas (habitable, garage, etc.) depende de la calidad de los terminados, como vimos en la introducción. En la siguiente receta de preprocesamiento:\n\nCortamos calidad general en 5 grupos: este paso no es necesario y puede dañar el desempeño, pero es consistente con el análisis que hicimos anteriormente.\nLidiamos con niveles nuevos y los ponemos en una categoría “nuevo” (para que nuestro modelo no falle al momento de predicción)\nPonemos los faltantes de calidad sotano y garage en una categoría nueva (no tienen sótano y/o garage)\nAgrupamos las zonas con pocas observaciones en una categoría de “Otros”\nQuitamos los NA’s de área garage y área sotano, que deben ser igual a 0 cuando no existen estas características.\nCreamos variables dummy de todas las variables categóricas\nIncluimos interacciones de distintas áreas con las dummy correspondientes, incluyendo zona con área habitable\nFinalmente, eliminamos para el ajuste aquellas variables que tengan varianza cercana a cero (500 /1 quiere decir que elimina cualquier variable cuyo conteo del valor más común entre el conteo de la siguiente es mayor a 500).\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gt)\nsource(\"../R/casas_traducir_geo.R\")\nset.seed(83)\ncasas_split <- initial_split(casas, prop = 0.75)\ncasas_entrena <- training(casas_split)\nreceta_casas <- recipe(precio_miles ~ \n           nombre_zona + \n           area_hab_m2 + area_garage_m2 + area_sotano_m2 + \n           area_lote_m2 + \n           año_construccion + \n           calidad_gral + calidad_garage + calidad_sotano + \n           num_coches  + \n           aire_acondicionado + condicion_venta, \n           data = casas_entrena) |> \n  step_filter(condicion_venta == \"Normal\") |> \n  step_select(-condicion_venta, skip = TRUE) |> \n  step_cut(calidad_gral, breaks = c(3, 5, 7, 8), \n           include_outside_range = TRUE) |>\n  step_novel(nombre_zona, calidad_sotano, calidad_garage) |> \n  step_unknown(calidad_sotano, calidad_garage) |> \n  step_other(nombre_zona, threshold = 0.02, other = \"otras\") |> \n  step_mutate(area_sotano_m2 = ifelse(is.na(area_sotano_m2), 0, area_sotano_m2)) |> \n  step_mutate(area_garage_m2 = ifelse(is.na(area_garage_m2), 0, area_garage_m2)) |> \n  step_dummy(nombre_zona, calidad_gral, calidad_garage, calidad_sotano, aire_acondicionado) |> \n  step_interact(terms = ~ area_hab_m2:starts_with(\"calidad_gral\")) |> \n  step_interact(terms = ~ area_hab_m2:starts_with(\"nombre_zona\")) |> \n  step_interact(terms = ~ area_garage_m2:starts_with(\"calidad_garage\")) |> \n  step_interact(terms = ~ area_sotano_m2: starts_with(\"calidad_sotano\")) |> \n  step_nzv(all_predictors(), freq_cut = 500 / 1, unique_cut = 1)\n\nEntrenamos la receta y vemos cuántos casos y columnas tenemos:\n\nreceta_casas_prep <- prep(receta_casas, verbose = TRUE)\n\noper 1 step filter [training] \noper 2 step select [training] \noper 3 step cut [training] \noper 4 step novel [training] \noper 5 step unknown [training] \noper 6 step other [training] \noper 7 step mutate [training] \noper 8 step mutate [training] \noper 9 step dummy [training] \noper 10 step interact [training] \noper 11 step interact [training] \noper 12 step interact [training] \noper 13 step interact [training] \noper 14 step nzv [training] \nThe retained training set is ~ 0.46 Mb  in memory.\n\ndatos_tbl <- juice(receta_casas_prep)\ndim(datos_tbl)\n\n[1] 907  62\n\n\n\ndatos_tbl |>\n  mutate(across(where(is.numeric), round, 2)) |>\n  head() |> \n  gt()\n\n\n\n\n\n  \n  \n    \n      area_hab_m2\n      area_garage_m2\n      area_sotano_m2\n      area_lote_m2\n      año_construccion\n      num_coches\n      precio_miles\n      nombre_zona_CollgCr\n      nombre_zona_Crawfor\n      nombre_zona_Edwards\n      nombre_zona_Gilbert\n      nombre_zona_IDOTRR\n      nombre_zona_Mitchel\n      nombre_zona_NAmes\n      nombre_zona_NoRidge\n      nombre_zona_NridgHt\n      nombre_zona_NWAmes\n      nombre_zona_OldTown\n      nombre_zona_Sawyer\n      nombre_zona_SawyerW\n      nombre_zona_Somerst\n      nombre_zona_Timber\n      nombre_zona_otras\n      calidad_gral_X.3.5.\n      calidad_gral_X.5.7.\n      calidad_gral_X.7.8.\n      calidad_gral_X.8.max.\n      calidad_garage_Fa\n      calidad_garage_Gd\n      calidad_garage_TA\n      calidad_garage_unknown\n      calidad_sotano_Fa\n      calidad_sotano_Gd\n      calidad_sotano_TA\n      calidad_sotano_unknown\n      aire_acondicionado_Y\n      area_hab_m2_x_calidad_gral_X.3.5.\n      area_hab_m2_x_calidad_gral_X.5.7.\n      area_hab_m2_x_calidad_gral_X.7.8.\n      area_hab_m2_x_calidad_gral_X.8.max.\n      area_hab_m2_x_nombre_zona_CollgCr\n      area_hab_m2_x_nombre_zona_Crawfor\n      area_hab_m2_x_nombre_zona_Edwards\n      area_hab_m2_x_nombre_zona_Gilbert\n      area_hab_m2_x_nombre_zona_IDOTRR\n      area_hab_m2_x_nombre_zona_Mitchel\n      area_hab_m2_x_nombre_zona_NAmes\n      area_hab_m2_x_nombre_zona_NoRidge\n      area_hab_m2_x_nombre_zona_NridgHt\n      area_hab_m2_x_nombre_zona_NWAmes\n      area_hab_m2_x_nombre_zona_OldTown\n      area_hab_m2_x_nombre_zona_Sawyer\n      area_hab_m2_x_nombre_zona_SawyerW\n      area_hab_m2_x_nombre_zona_Somerst\n      area_hab_m2_x_nombre_zona_Timber\n      area_hab_m2_x_nombre_zona_otras\n      area_garage_m2_x_calidad_garage_Fa\n      area_garage_m2_x_calidad_garage_Gd\n      area_garage_m2_x_calidad_garage_TA\n      area_sotano_m2_x_calidad_sotano_Fa\n      area_sotano_m2_x_calidad_sotano_Gd\n      area_sotano_m2_x_calidad_sotano_TA\n    \n  \n  \n    137.22\n20.07\n79.99\n584.55\n1928\n1\n145.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n0.00\n137.22\n0.00\n0\n0.00\n0\n0\n0.00\n0\n0\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n137.22\n0\n0\n20.07\n0\n0.00\n79.99\n    179.86\n46.82\n130.62\n1039.96\n2000\n2\n230.5\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n0.00\n0.00\n179.86\n0\n0.00\n0\n0\n179.86\n0\n0\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n0.00\n0\n0\n46.82\n0\n130.62\n0.00\n    81.94\n27.31\n0.00\n774.72\n1959\n1\n106.5\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n1\n81.94\n0.00\n0.00\n0\n0.00\n0\n0\n0.00\n0\n0\n81.94\n0\n0\n0\n0\n0\n0\n0\n0\n0.00\n0\n0\n27.31\n0\n0.00\n0.00\n    153.01\n20.07\n74.88\n637.13\n1915\n1\n128.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0.00\n153.01\n0.00\n0\n0.00\n0\n0\n0.00\n0\n0\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n153.01\n0\n0\n20.07\n0\n74.88\n0.00\n    101.45\n26.57\n50.73\n205.97\n1970\n1\n88.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n101.45\n0.00\n0.00\n0\n0.00\n0\n0\n0.00\n0\n0\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n101.45\n0\n0\n26.57\n0\n0.00\n50.73\n    71.35\n36.79\n71.35\n668.90\n1972\n1\n133.9\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n71.35\n0.00\n0.00\n0\n71.35\n0\n0\n0.00\n0\n0\n0.00\n0\n0\n0\n0\n0\n0\n0\n0\n0.00\n0\n0\n36.79\n0\n71.35\n0.00\n  \n  \n  \n\n\n\n\nFinalmente, usamos un modelo lineal con las 62 entradas que acabamos de crear:\n\nflujo_casas <- workflow() |> \n  add_recipe(receta_casas) |> \n  add_model(linear_reg() |> set_engine(\"lm\"))\najuste <- fit(flujo_casas, casas_entrena)\n\nAunque no es de interés particular para nosotros por el momento, examinamos los coeficientes (que no son tan simples de interpretar como discutiremos más adelante):\n\najuste |> broom::tidy() |> \n  mutate(across(where(is.numeric), round, 2)) |> \n  select(term, estimate) |> \n  gt()\n\n\n\n\n\n  \n  \n    \n      term\n      estimate\n    \n  \n  \n    (Intercept)\n-858.68\n    area_hab_m2\n0.80\n    area_garage_m2\n1.02\n    area_sotano_m2\n0.88\n    area_lote_m2\n0.01\n    año_construccion\n0.39\n    num_coches\n2.11\n    nombre_zona_CollgCr\n16.70\n    nombre_zona_Crawfor\n37.11\n    nombre_zona_Edwards\n28.82\n    nombre_zona_Gilbert\n31.31\n    nombre_zona_IDOTRR\n11.81\n    nombre_zona_Mitchel\n32.01\n    nombre_zona_NAmes\n28.74\n    nombre_zona_NoRidge\n22.00\n    nombre_zona_NridgHt\n-6.51\n    nombre_zona_NWAmes\n17.67\n    nombre_zona_OldTown\n25.28\n    nombre_zona_Sawyer\n37.91\n    nombre_zona_SawyerW\n-11.95\n    nombre_zona_Somerst\n-8.99\n    nombre_zona_Timber\n3.23\n    nombre_zona_otras\n-14.27\n    calidad_gral_X.3.5.\n35.54\n    calidad_gral_X.5.7.\n13.76\n    calidad_gral_X.7.8.\n16.99\n    calidad_gral_X.8.max.\n-67.26\n    calidad_garage_Fa\n15.54\n    calidad_garage_Gd\n39.04\n    calidad_garage_TA\n18.71\n    calidad_garage_unknown\n20.61\n    calidad_sotano_Fa\n72.92\n    calidad_sotano_Gd\n54.95\n    calidad_sotano_TA\n60.56\n    calidad_sotano_unknown\n59.88\n    aire_acondicionado_Y\n15.17\n    area_hab_m2_x_calidad_gral_X.3.5.\n-0.25\n    area_hab_m2_x_calidad_gral_X.5.7.\n0.05\n    area_hab_m2_x_calidad_gral_X.7.8.\n0.19\n    area_hab_m2_x_calidad_gral_X.8.max.\n0.81\n    area_hab_m2_x_nombre_zona_CollgCr\n-0.25\n    area_hab_m2_x_nombre_zona_Crawfor\n-0.14\n    area_hab_m2_x_nombre_zona_Edwards\n-0.39\n    area_hab_m2_x_nombre_zona_Gilbert\n-0.33\n    area_hab_m2_x_nombre_zona_IDOTRR\n-0.21\n    area_hab_m2_x_nombre_zona_Mitchel\n-0.42\n    area_hab_m2_x_nombre_zona_NAmes\n-0.31\n    area_hab_m2_x_nombre_zona_NoRidge\n-0.16\n    area_hab_m2_x_nombre_zona_NridgHt\n-0.03\n    area_hab_m2_x_nombre_zona_NWAmes\n-0.26\n    area_hab_m2_x_nombre_zona_OldTown\n-0.32\n    area_hab_m2_x_nombre_zona_Sawyer\n-0.43\n    area_hab_m2_x_nombre_zona_SawyerW\n-0.07\n    area_hab_m2_x_nombre_zona_Somerst\n0.00\n    area_hab_m2_x_nombre_zona_Timber\n-0.14\n    area_hab_m2_x_nombre_zona_otras\n0.00\n    area_garage_m2_x_calidad_garage_Fa\n-0.59\n    area_garage_m2_x_calidad_garage_Gd\n-0.97\n    area_garage_m2_x_calidad_garage_TA\n-0.78\n    area_sotano_m2_x_calidad_sotano_Fa\n-0.86\n    area_sotano_m2_x_calidad_sotano_Gd\n-0.53\n    area_sotano_m2_x_calidad_sotano_TA\n-0.67\n  \n  \n  \n\n\n\n\nNótese que:\n\nEn esta tabla están los coeficientes \\(\\beta_i\\) en las covariables que creamos a partir de las variables de entrada.\nEl modelo lineal no tiene que ser lineal en las variables que recibimos originalmente en la tabla de datos.\nEn este ejemplo, convertimos algunas variables a dummy, y multiplicamos algunas variables de área por esas variables dummy.\n\nFinalmente, evaluamos el desempeño sobre las ventas normales:\n\nmetricas <- metric_set(mape, mae, rmse, rsq)\ncasas_prueba_normal <- testing(casas_split) |> \n  filter(condicion_venta == \"Normal\")\nmetricas(casas_prueba_normal |> bind_cols(predict(ajuste, casas_prueba_normal)), \n     truth = precio_miles, estimate = .pred) |> \n  gt() |> fmt_number(.estimate, decimals = 2)\n\n\n\n\n\n  \n  \n    \n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    mape\nstandard\n10.34\n    mae\nstandard\n17.12\n    rmse\nstandard\n23.49\n    rsq\nstandard\n0.89"
  },
  {
    "objectID": "04-lineales-ingenieria.html#no-linealidad-y-atípicos",
    "href": "04-lineales-ingenieria.html#no-linealidad-y-atípicos",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.6 No linealidad y atípicos",
    "text": "4.6 No linealidad y atípicos\nEn un primer ejemplo consideremos la variable de area de lote:\n\nlibrary(patchwork)\ng_1 <- ggplot(casas_entrena, aes(x = area_lote_m2, y = precio_miles)) +\n  geom_point() + #geom_smooth(method = \"loess\", span = 0.5, se = FALSE,\n                #             method.args = list(degree = 1)) +\n  geom_smooth(method = \"lm\", se = FALSE)\ng_2 <- ggplot(casas_entrena |> filter(area_lote_m2 < 5000), aes(x = area_lote_m2, y = precio_miles)) +\n  geom_point(data = casas_entrena, aes(colour = area_lote_m2 > 5000)) + \n  #geom_smooth(method = \"loess\", span = 0.5, se = FALSE,\n  #                           method.args = list(degree = 1)) +\n  geom_smooth(method = \"lm\", se = FALSE)\ng_1 + g_2\n\n\n\n\nY notamos que hay algunos valores grandes que pueden perturbar el ajuste lineal. Esto puede producir varianza alta en las predicciones, pues el ajuste depende mucho de unos cuantos valores de entrenamiento. Una solución puede ser transformar la entrada por ejempo usando el logaritmo, que comprime la cola derecha de la distribución de la variable que tiene mucho sesgo:\n\nggplot(casas_entrena, aes(x = area_lote_m2, y = precio_miles)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", span = 0.5, se = FALSE) +\n  scale_x_log10()\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nNótese que probablemente tendremos que agregar más flexibilidad en nuestro predictor para capturar apropiadamente la información en esta variable."
  },
  {
    "objectID": "04-lineales-ingenieria.html#no-linealidad-y-splines",
    "href": "04-lineales-ingenieria.html#no-linealidad-y-splines",
    "title": "4  Métodos lineales e ingenería de entradas",
    "section": "4.7 No linealidad y splines",
    "text": "4.7 No linealidad y splines\nEn algunos casos, la relación de una variable de entrada con la predicción es no lineal. Podemos entonces incluír entradas derivadas de la original usando transformaciones no lineales: por ejemplo, transformar entradas usando el logaritmo, o agregar el cuadrado o la raíz de las variables de entrada.\nUna de las maneras más simples y menos problemáticas de hacer esto es usando splines naturales para modelar, que son funciones cúbicas por tramos dos veces diferenciables. Los tramos están definidos por nudos que podemos definir por ejemplo igualmente espaciados en los datos.\n\nvalores_x <- seq(-10, 110, 1)\nbase_splines <- splines::ns(x = valores_x, knots = c(33, 66), \n                            Boundary.knots = c(0, 100))\nspline_1 <- base_splines %*% c(1, 1, 1)\nspline_2 <- base_splines %*% c(-0.1, 2, 1)\ntibble(x = valores_x, y = spline_1, spline = 1) |> \nbind_rows(tibble(x = valores_x, y = spline_2, spline = 2)) |> \n  ggplot(aes(x = x, y = y, group = spline, colour = factor(spline))) + \n  geom_point() + geom_line() +\n  geom_vline(xintercept = c(0, 33, 66, 100), colour = \"red\")\n\n\n\n\nEstos dos son ejemplos de funciones cúbicas por tramos y dos veces diferenciables, con nudos en 0, 33, 66 y 100. Su forma particular depende de tres coeficientes, que pueden pensarse también como definidos por dónde tienen que pasar la curva en \\(y\\) para los valores \\(x = 33, 66\\) y \\(100\\). Extrapolan linealmente fuera del rango de los datos.\nLa ventaja de utilizar estos splines es que son estables en el cálculo, pues a lo más utilizan potencias cúbicas, y la complejidad puede aumentarse incrementando el número de nodos.\n\nEjemplo\nRevisamos nuestro ejemplo de rendimiento de coches:\n\nlibrary(tidyverse)\nlibrary(gt)\nauto <- read_csv(\"../datos/auto.csv\")\ndatos <- auto[, c('name', 'weight','year', 'mpg', 'displacement')]\ndatos <- datos %>% mutate(\n  peso_kg = weight * 0.45359237,\n  rendimiento_kpl = mpg * (1.609344 / 3.78541178), \n  año = year)\n\nVamos a separa en muestra de entrenamiento y de prueba estos datos. Podemos hacerlo como sigue (75% para entrenamiento aproximadamente en este caso, así obtenemos alrededor de 100 casos para prueba):\n\nlibrary(tidymodels)\nset.seed(121)\ndatos_split <- initial_split(datos, prop = 0.75)\ndatos_entrena <- training(datos_split)\ndatos_prueba <- testing(datos_split)\n\nVamos a usar año y peso de los coches para predecir su rendimiento:\n\nggplot(datos_entrena, aes(x = peso_kg, y = rendimiento_kpl, colour = año)) +\n  geom_point()\n\n\n\n\nNuestra receta incluye la transformación no lineal de splines:\n\nreceta_lineal <- recipe(rendimiento_kpl ~ peso_kg + año, datos_entrena) |> \n  step_ns(peso_kg, deg_free = 3) |> \n  step_ns(año, deg_free = 2)\nmod_lineal <- linear_reg() |>  \n  set_engine(\"lm\")  \nflujo <- workflow() |>  \n  add_recipe(receta_lineal) |> \n  add_model(mod_lineal)\n\nLos datos de entrada son los siguientes:\n\njuice(prep(receta_lineal)) |> head() |> gt()\n\n\n\n\n\n  \n  \n    \n      rendimiento_kpl\n      peso_kg_ns_1\n      peso_kg_ns_2\n      peso_kg_ns_3\n      año_ns_1\n      año_ns_2\n    \n  \n  \n    12.754311\n-0.1384501\n0.3919536\n-0.2338715\n0.1263158\n-0.08343893\n    13.136941\n-0.1508427\n0.4978146\n-0.2970367\n0.5652102\n0.00590925\n    12.754311\n0.3794499\n0.4646633\n-0.2232209\n0.4765544\n0.35513660\n    7.695101\n0.4471996\n0.4245669\n-0.1625354\n0.5652102\n0.00590925\n    8.757960\n0.4368600\n0.4313780\n-0.1743974\n0.5652102\n0.00590925\n    14.242314\n-0.1126420\n0.2985773\n-0.1781555\n0.5794245\n-0.12316573\n  \n  \n  \n\n\n\n\nNótese que tenemos 4 entradas en lugar de las 2 originales, pues creamos dos transformaciones no lineales de peso_kg. El modelo es lineal en estas 4 variables, pero no en las 2 originales. Ajustamos:\n\nflujo_ajustado <- fit(flujo, datos_entrena)\n\nY ahora podemos graficar los resultados y vemos cómo pudimos capturar la relación no lineal entre peso y rendimiento:\n\ndat_graf <- tibble(peso_kg = seq(900, 2200, by = 10)) %>% \n  crossing(tibble(año = c(70, 75, 80)))\ndat_graf <- dat_graf %>% \n  mutate(pred_1 = predict(flujo_ajustado, dat_graf) %>% pull(.pred))\nggplot(datos_entrena, aes(x = peso_kg, group = año, colour = año)) +\n  geom_point(aes(y = rendimiento_kpl), alpha = 0.6) + \n  geom_line(data = dat_graf, aes(y = pred_1),  size = 1.2)\n\n\n\n\nLos grados de libertad también pueden afinarse utilizando un conjunto de validación como hicimos antes en vecinos más cercanos.\n\n\nEjemplo: casas\nPor ejemplo, podríamos incluir un efecto no lineal de area_lote, calidad y condición general y año de construcción:\n\nreceta_casas <- recipe(precio_miles ~ \n           nombre_zona + \n           area_hab_m2 + area_garage_m2 + area_sotano_m2 + \n           area_lote_m2 + \n           año_construccion + \n           calidad_gral + calidad_garage + calidad_sotano + \n           condicion_gral + \n           num_coches  + \n           aire_acondicionado + condicion_venta, \n           data = casas_entrena) |> \n  step_filter(condicion_venta == \"Normal\") |> \n  step_select(-condicion_venta, skip = TRUE) |> \n  \n  step_novel(nombre_zona, calidad_sotano, calidad_garage) |> \n  step_unknown(calidad_sotano, calidad_garage) |> \n  step_other(nombre_zona, threshold = 0.02, other = \"otras\") |> \n  step_mutate(area_sotano_m2 = ifelse(is.na(area_sotano_m2), 0, area_sotano_m2)) |> \n  step_mutate(area_garage_m2 = ifelse(is.na(area_garage_m2), 0, area_garage_m2)) |>\n # step_log(area_lote_m2) |> \n  step_ns(año_construccion, deg_free = 2) |> \n  step_ns(calidad_gral, deg_free = 2) |> \n  step_ns(condicion_gral, deg_free = 2) |> \n  step_ns(area_lote_m2, deg_free = 3) |> \n  step_dummy(nombre_zona,  calidad_garage, calidad_sotano, aire_acondicionado) |> \n  step_interact(terms = ~ area_hab_m2:starts_with(\"calidad_gral\")) |> \n  step_interact(terms = ~ area_hab_m2:starts_with(\"nombre_zona\")) |> \n  step_interact(terms = ~ area_garage_m2:starts_with(\"calidad_garage\")) |> \n  step_interact(terms = ~ area_sotano_m2: starts_with(\"calidad_sotano\")) |> \n  step_nzv(all_predictors(), freq_cut = 500 / 1, unique_cut = 1)\n\n\nflujo_casas <- workflow() |> \n  add_recipe(receta_casas) |> \n  add_model(linear_reg() |> set_engine(\"lm\"))\najuste <- fit(flujo_casas, casas_entrena)\n\nFinalmente, evaluamos el desempeño sobre las ventas normales, y obtenemos una mejoría con respecto a nuestro modelo anterior:\n\nmetricas <- metric_set(mape, mae, rmse, rsq)\ncasas_prueba_normal <- testing(casas_split) |> \n  filter(condicion_venta == \"Normal\")\nmetricas(casas_prueba_normal |> bind_cols(predict(ajuste, casas_prueba_normal)), \n     truth = precio_miles, estimate = .pred) |> \n  gt() |> fmt_number(.estimate, decimals = 2)\n\n\n\n\n\n  \n  \n    \n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    mape\nstandard\n8.47\n    mae\nstandard\n14.45\n    rmse\nstandard\n19.77\n    rsq\nstandard\n0.92\n  \n  \n  \n\n\n\n\nFinalmente, examinamos la respuesta contra la predicción:\n\ncasas_prueba_normal |> bind_cols(predict(ajuste, casas_prueba_normal)) |> \n  ggplot(aes(x = .pred, y = precio_miles)) + geom_abline() + \n    geom_point(colour = \"red\") + coord_obs_pred()"
  },
  {
    "objectID": "99-referencias.html",
    "href": "99-referencias.html",
    "title": "Referencias",
    "section": "",
    "text": "Bishop, Christopher M. 2006. Pattern Recognition and Machine\nLearning (Information Science and Statistics). Secaucus, NJ, USA:\nSpringer-Verlag New York, Inc.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep\nLearning. MIT Press.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. The\nElements of Statistical Learning. Springer Series in Statistics.\nSpringer New York Inc. http://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2014. An Introduction to Statistical Learning: With Applications in\nr. Springer Publishing Company, Incorporated. http://www-bcf.usc.edu/~gareth/ISL/.\n\n\nKuhn, M., and J. Silge. 2022. Tidy Modeling with r. O’Reilly\nMedia. https://books.google.com.mx/books?id=9cJ6EAAAQBAJ."
  }
]